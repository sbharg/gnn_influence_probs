{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, Linear, SAGEConv, GATv2Conv, GATConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy as sp\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeGNN(nn.Module):\n",
    "    def __init__(self, num_nodes, num_edges, node_dim=64, hidden_dim=16, num_layers=2):\n",
    "        super(CascadeGNN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_edges = num_edges\n",
    "        self.node_dim = node_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initial node embedding layer\n",
    "        self.node_embedding = nn.Embedding(self.num_nodes, self.node_dim)\n",
    "        #self.edge_emb = nn.Parameter(torch.Tensor(num_nodes, num_nodes, hidden_dim))\n",
    "        self.edge_embedding = nn.Embedding(self.num_edges, self.hidden_dim)\n",
    "\n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(self.node_dim if i == 0 else hidden_dim, hidden_dim) \n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "            \n",
    "        # Edge probability prediction layer\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, edge_index):\n",
    "        # Get initial node embeddings\n",
    "        x = self.node_embedding(torch.arange(self.num_nodes))\n",
    "        #src, dst = edge_index\n",
    "        #edge_arr = self.edge_emb[src, dst]\n",
    "        edge_emb = self.edge_embedding(torch.arange(self.num_edges))\n",
    "\n",
    "        # Apply GNN layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr=edge_emb)\n",
    "            x = F.gelu(x)\n",
    "            x = F.dropout(x, p=0.1, training=self.training)\n",
    "            \n",
    "        # Compute edge probabilities for all edges\n",
    "        edge_probabilities = {}\n",
    "        for i in range(edge_index.size(1)):\n",
    "            source, target = edge_index[:, i]\n",
    "            #edge_repr = torch.cat([torch.add(x[source], x[target]), edge_emb[i]], dim=0)\n",
    "            edge_repr = torch.cat([x[source], x[target], edge_emb[i]], dim=0)\n",
    "            prob = self.edge_mlp(edge_repr)\n",
    "            edge_probabilities[(source.item(), target.item())] = prob\n",
    "        \n",
    "        return edge_probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cascade_likelihood(edge_probs, adj_list, cascade, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood of observing a cascade given edge probabilities\n",
    "    \n",
    "    Args:\n",
    "        num_nodes: Number of nodes in the graph\n",
    "        edge_probs: Dictionary mapping (source, target) tuples to probabilities\n",
    "        cascade: List of lists, where cascade[i] contains nodes activated at time i\n",
    "        eps: Small value to prevent log(0)\n",
    "    \n",
    "    Returns:\n",
    "        Negative log likelihood of the cascade\n",
    "    \"\"\"\n",
    "    log_likelihood = 0.0\n",
    "    activated_nodes = set()\n",
    "    \n",
    "    # Process each time step\n",
    "    for t in range(len(cascade)):\n",
    "        prev_activated = cascade[t-1] if t-1 >= 0 else []\n",
    "        curr_activated = cascade[t]\n",
    "        next_activated = cascade[t+1] if t+1 < len(cascade) else []\n",
    "        activated_nodes.update(curr_activated)\n",
    "\n",
    "        #print(t)\n",
    "        #print(prev_activated)\n",
    "        #print(curr_activated)\n",
    "        #print(next_activated)\n",
    "\n",
    "        for v in curr_activated:\n",
    "            # Probability of activation from parents\n",
    "            if prev_activated:\n",
    "                #parents = set([u for u in range(num_nodes) if (u, v) in edge_probs and u in prev_activated])\n",
    "                parents = adj_list[v][0]\n",
    "                activated_parents = parents.intersection(set(prev_activated))\n",
    "                prob = [1 - edge_probs[(u, v)] for u in activated_parents]\n",
    "                prob = torch.cat(prob)\n",
    "                prob_not_activated = torch.prod(prob)\n",
    "                log_likelihood += torch.log(1 - prob_not_activated + eps)\n",
    "            if next_activated:\n",
    "                children = adj_list[v][1]\n",
    "                non_activated_children = children.difference(activated_nodes).difference(set(next_activated))\n",
    "                #children = set([w for w in range(num_nodes) if (v, w) in edge_probs and w not in activated_nodes and w not in set(next_activated)])\n",
    "                if not non_activated_children:\n",
    "                    continue\n",
    "                prob = [1 - edge_probs[(v, w)] for w in non_activated_children]\n",
    "                prob = torch.cat(prob)\n",
    "                #print(prob)\n",
    "                prob_not_activated = torch.prod(prob)\n",
    "                log_likelihood += torch.log(prob_not_activated + eps)\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def compute_loss(edge_probs, adj_list, cascades, eps=1e-6):\n",
    "  \"\"\"\n",
    "  Compute the negative log-likelihood loss for multiple cascades.\n",
    "  \n",
    "  Args:\n",
    "    num_nodes: Number of nodes in the\n",
    "    edge_probs: Tensor of predicted edge probabilities\n",
    "    edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "    cascades: List of cascades, where each cascade is a list of lists of activated nodes\n",
    "  \n",
    "  Returns:\n",
    "    loss: Negative log-likelihood loss\n",
    "  \"\"\"\n",
    "  total_log_likelihood = 0.0\n",
    "  for cascade in cascades:\n",
    "    total_log_likelihood += compute_cascade_likelihood(edge_probs, adj_list, cascade, eps=eps)\n",
    "  \n",
    "  # Return negative log-likelihood as the loss\n",
    "  #print(-total_log_likelihood)\n",
    "  return -total_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cascade_gnn(model, edge_index, cascades, adj_list, num_epochs=100, batch_size = 50, lr=0.001, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the GNN model using the observed cascades\n",
    "    \n",
    "    Args:\n",
    "        model: CascadeGNN model\n",
    "        num_nodes: Number of nodes in the graph\n",
    "        edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "        cascades: List of cascades, where each cascade is a list of lists\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0.0\n",
    "        rng.shuffle(cascades)\n",
    "        batches = [cascades[i:i+batch_size] for i in range(0, len(cascades), batch_size)]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch in batches:\n",
    "            optimizer.zero_grad()\n",
    "            # Get edge probabilities\n",
    "            edge_probs = model.forward(edge_index)\n",
    "\n",
    "            batch_loss = compute_loss(edge_probs, adj_list, batch)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss += batch_loss.item()\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(G: nx.DiGraph):\n",
    "  edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "  adj_list = {v : (set(), set()) for v in G.nodes}\n",
    "  for e in G.edges:\n",
    "    u, v = e\n",
    "    adj_list[u][1].add(v)\n",
    "    adj_list[v][0].add(u)\n",
    "  return edge_index, adj_list\n",
    "\n",
    "#n = 100\n",
    "#p = 0.2\n",
    "#gname = f\"er_{n}_{str(p).replace('.', '')}\"\n",
    "#gname = \"ba_500_2\"\n",
    "#gname = \"sf_1500_041_054_005\"\n",
    "gname = \"ego-facebook\"\n",
    "#gname = \"er_100_01\"\n",
    "path = Path(f\"datasets/real/{gname}\")\n",
    "\n",
    "with open(path / f\"graph.mtx\", \"rb\") as fh:\n",
    "  G = nx.from_scipy_sparse_array(sp.io.mmread(fh), create_using=nx.DiGraph)\n",
    "\n",
    "cascades = []\n",
    "for i in range(250):\n",
    "  with open(path / f\"diffusions/timestamps/{i}.txt\", \"r\") as fh:\n",
    "    cascade = []\n",
    "    for line in fh:\n",
    "      cascade.append(list(map(int, line.strip().split())))\n",
    "    cascades.append(cascade)\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "edge_index, adj_list = create_dataset(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for c in cascades:\n",
    "    #print(c)\n",
    "    max_len = max(max_len, len(c))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\tMAE\t\t\tTime\n",
      "50\t0.07867444117853627\t12084.335420846939\n",
      "100\t0.13385399025532807\t24701.671295404434\n"
     ]
    }
   ],
   "source": [
    "l1_errors = []\n",
    "times = []\n",
    "#cascade_sizes = [50, 100, 150]\n",
    "cascade_sizes = [50, 100, 150, 200, 250]\n",
    "results_path = Path(f\"results/gnn/\", )\n",
    "\n",
    "print(\"M\\tMAE\\t\\t\\tTime\")\n",
    "with open(results_path / \"gnn.txt\", \"a\") as fh:\n",
    "        fh.write(f\"\\nGraph: {gname}\\n\")\n",
    "        fh.write(\"M\\t\\tMAE\\t\\t\\t\\t\\t\\tTime\\n\")\n",
    "\n",
    "for k in cascade_sizes:\n",
    "    start = time.time()\n",
    "    model = CascadeGNN(n, m, hidden_dim=16, num_layers=2)\n",
    "    #trained_model = \n",
    "    train_cascade_gnn(model, edge_index, cascades[:k], adj_list, num_epochs=35, lr=0.01, verbose=False)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "    model.eval()\n",
    "    edge_probs = model(edge_index)\n",
    "    residuals = []\n",
    "    H = nx.DiGraph()\n",
    "    for i, e in enumerate(G.edges()):\n",
    "        u, v = e\n",
    "        p = G[u][v]['weight']\n",
    "        residuals.append(abs(p - edge_probs[e].item()))\n",
    "        H.add_edge(u, v, weight=edge_probs[e].item())\n",
    "\n",
    "    l1_err = sum(residuals) / len(residuals)\n",
    "    l1_errors.append(l1_err)\n",
    "    print(f\"{k}\\t{l1_err}\\t{end-start}\")\n",
    "    with open(results_path / \"gnn.txt\", \"a\") as fh:\n",
    "        fh.write(f\"{k}\\t\\t{l1_err}\\t\\t{end-start}\\n\")\n",
    "\n",
    "    graph_path = results_path / f\"{gname}_{k}.mtx\"\n",
    "    with graph_path.open('wb') as fh:\n",
    "        mat = nx.to_scipy_sparse_array(H)\n",
    "        sp.io.mmwrite(fh, mat, precision=5)\n",
    "\n",
    "#rint(\"M\\tMAE\\t\\t\\tTime\")\n",
    "#for i, k in enumerate(cascade_sizes):\n",
    "#    print(f\"{k}\\t{l1_errors[i]}\\t{times[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11266553486120538, 0.11358482052435794, 0.11321353166009078]\n",
      "[53.8030731678009, 100.08372330665588, 145.98774337768555]\n"
     ]
    }
   ],
   "source": [
    "print(l1_errors)\n",
    "print(times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
