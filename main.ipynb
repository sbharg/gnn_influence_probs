{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbharg/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/Users/sbharg/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.12/Python\n",
      "  Referenced from: <E87A820F-D734-3F45-AFBE-9D80043A97C0> /Users/sbharg/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, Linear, SAGEConv, GATv2Conv, GATConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy as sp\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cascade_likelihood(edge_probs, edge_index, cascade, epsilon=1e-8):\n",
    "  \"\"\"\n",
    "  Compute the likelihood of observing a single cascade given edge probabilities.\n",
    "  \n",
    "  Args:\n",
    "  - edge_probs: Tensor of predicted edge probabilities\n",
    "  - edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "  - cascade: List of lists, where each inner list contains nodes activated at that time step\n",
    "  - epsilon: Small value to avoid log(0)\n",
    "  \n",
    "  Returns:\n",
    "  - log_likelihood: Log-likelihood of the cascade\n",
    "  \"\"\"\n",
    "  device = edge_probs.device\n",
    "  num_nodes = edge_index.max().item() + 1\n",
    "  activated = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "  log_likelihood = 0.0\n",
    "\n",
    "  src, dst = edge_index\n",
    "\n",
    "  for t in range(len(cascade)):\n",
    "    prev_activated = torch.tensor(cascade[t-1] if t-1 >= 0 else [], device=device)\n",
    "    curr_activated = torch.tensor(cascade[t], device=device)\n",
    "    next_activated = torch.tensor(cascade[t+1] if t+1 < len(cascade) else [], device=device)\n",
    "    activated[curr_activated] = True\n",
    "    \n",
    "    # Probability of activation from parents\n",
    "    for v in curr_activated:\n",
    "      parents = src[(dst == v) & activated[src]]\n",
    "      activated_parents = parents[torch.isin(parents, prev_activated)]\n",
    "      if len(activated_parents) > 0:\n",
    "        prob_v_activated = 1 - torch.prod(1 - edge_probs[torch.isin(src, activated_parents) & (dst == v)])\n",
    "        log_likelihood += torch.log(prob_v_activated + epsilon)\n",
    "\n",
    "    \n",
    "    # Probability of non-activation of children\n",
    "    for v in curr_activated:\n",
    "      children = dst[(src == v) & ~activated[dst]]\n",
    "      non_activated_children = children[~torch.isin(children, next_activated)]\n",
    "      if len(non_activated_children) > 0:\n",
    "        prob_children_not_activated = torch.prod(1 - edge_probs[(src == v) & torch.isin(dst, non_activated_children)])\n",
    "        log_likelihood += torch.log(prob_children_not_activated + epsilon)\n",
    "    \n",
    "\n",
    "  return log_likelihood\n",
    "\n",
    "  '''\n",
    "  device = edge_probs.device\n",
    "  num_nodes = edge_index.max().item() + 1\n",
    "  activated = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "  log_likelihood = 0.0\n",
    "\n",
    "  for t, activated_nodes in enumerate(cascade):\n",
    "    if t == 0:\n",
    "      activated[activated_nodes] = True\n",
    "      continue\n",
    "\n",
    "    # Compute activation probabilities for this step\n",
    "    src, dst = edge_index\n",
    "    mask = activated[src] & ~activated[dst]\n",
    "    relevant_probs = edge_probs[mask]\n",
    "    relevant_dst = dst[mask]\n",
    "\n",
    "    # Compute likelihood of activations and non-activations\n",
    "    new_activations = torch.tensor(activated_nodes, device=device)\n",
    "    activated_probs = relevant_probs[torch.isin(relevant_dst, new_activations)]\n",
    "    non_activated_probs = relevant_probs[~torch.isin(relevant_dst, new_activations)]\n",
    "\n",
    "    log_likelihood += torch.sum(torch.log(activated_probs + epsilon))\n",
    "    log_likelihood += torch.sum(torch.log(1 - non_activated_probs + epsilon))\n",
    "\n",
    "    # Update activated nodes\n",
    "    activated[activated_nodes] = True\n",
    "\n",
    "  return log_likelihood\n",
    "  '''\n",
    "\n",
    "def compute_loss(edge_probs, edge_index, cascades):\n",
    "  \"\"\"\n",
    "  Compute the negative log-likelihood loss for multiple cascades.\n",
    "  \n",
    "  Args:\n",
    "  - edge_probs: Tensor of predicted edge probabilities\n",
    "  - edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "  - cascades: List of cascades, where each cascade is a list of lists of activated nodes\n",
    "  \n",
    "  Returns:\n",
    "  - loss: Negative log-likelihood loss\n",
    "  \"\"\"\n",
    "  total_log_likelihood = 0.0\n",
    "  for cascade in cascades:\n",
    "    total_log_likelihood += compute_cascade_likelihood(edge_probs, edge_index, cascade)\n",
    "  \n",
    "  # Return negative log-likelihood as the loss\n",
    "  return -total_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(G: nx.DiGraph, features):\n",
    "  # Create a PyG Data object from the networkx graph\n",
    "  edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "  #x = torch.tensor(features, dtype=torch.float)\n",
    "  data = Data(x=features, edge_index=edge_index)\n",
    "  return data\n",
    "\n",
    "n = 100\n",
    "p = 0.1\n",
    "gname = f\"er_{n}_{str(p).replace('.', '')}\"\n",
    "path = Path(f\"datasets/synthetic/{gname}\")\n",
    "\n",
    "with open(path / f\"{gname}.mtx\", \"rb\") as fh:\n",
    "  G = nx.from_scipy_sparse_array(sp.io.mmread(fh), create_using=nx.DiGraph)\n",
    "with open(path / \"feats.npy\", \"rb\") as fh:\n",
    "  features_npy = torch.tensor(np.load(fh), dtype=torch.float)\n",
    "\n",
    "cascades = []\n",
    "idxes = rng.choice(500, 100, replace=False)\n",
    "for i in idxes:\n",
    "  with open(path / f\"diffusions/timestamps/{i}.txt\", \"r\") as fh:\n",
    "    cascade = []\n",
    "    for line in fh:\n",
    "      cascade.append(list(map(int, line.strip().split())))\n",
    "    cascades.append(cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Total Loss: 12887.4165\n",
      "Epoch 11/100, Total Loss: 11622.9136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m GNNIndependentCascade(data\u001b[38;5;241m.\u001b[39mnum_features, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     62\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcascades\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, data, cascades, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m edge_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(data)\n\u001b[1;32m     48\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m compute_loss(edge_probs, data\u001b[38;5;241m.\u001b[39medge_index, batch)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/homework/ut_austin/ece381k_mlnetworks/gnn_influence_probs/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GNNIndependentCascade(torch.nn.Module):\n",
    "  def __init__(self, num_node_features, hidden_dim, num_layers=2):\n",
    "    super(GNNIndependentCascade, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.convs = nn.ModuleList([\n",
    "      GATConv(num_node_features if i == 0 else hidden_dim, hidden_dim) \n",
    "      for i in range(num_layers)]\n",
    "    )\n",
    "\n",
    "    self.edge_predictor = nn.Sequential(\n",
    "      nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "      nn.ELU(),\n",
    "      nn.Linear(hidden_dim, 1),\n",
    "      nn.ELU()\n",
    "    )\n",
    "\n",
    "  def forward(self, data):\n",
    "    x, edge_index = data.x, data.edge_index\n",
    "\n",
    "    # Node embedding\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.convs[i](x, edge_index)\n",
    "      x = torch.relu(x)\n",
    "      x = torch.dropout(x, p=0.1, train=self.training)\n",
    "\n",
    "    # Edge probability prediction\n",
    "    row, col = edge_index\n",
    "    edge_features = torch.cat([x[row], x[col]], dim=1)\n",
    "    edge_probs = torch.sigmoid(self.edge_predictor(edge_features).squeeze())\n",
    "    #edge_probs = torch.sigmoid(torch.sum(x[row] * x[col], dim=1))\n",
    "\n",
    "    return edge_probs\n",
    "\n",
    "def train_model(model, optimizer, data, cascades, num_epochs, batch_size = 50):\n",
    "  model.train()\n",
    "  #batches = DataLoader(cascades, batch_size=10, shuffle=True)\n",
    "  #print(batches)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    loss = 0.0\n",
    "    rng.shuffle(cascades)\n",
    "    batches = [cascades[i:i+batch_size] for i in range(0, len(cascades), batch_size)]\n",
    "\n",
    "    for batch in batches:\n",
    "      optimizer.zero_grad()\n",
    "      edge_probs = model.forward(data)\n",
    "\n",
    "      batch_loss = compute_loss(edge_probs, data.edge_index, batch)\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      loss += batch_loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {loss:.4f}\")\n",
    "      #print(edge_probs[0])\n",
    "      #print(edge_probs[1])\n",
    "      #print('\\n')\n",
    "\n",
    "features_eye = torch.eye(G.number_of_nodes())\n",
    "data = create_dataset(G, features_eye)\n",
    "model = GNNIndependentCascade(data.num_features, 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "train_model(model, optimizer, data, cascades, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.2186086857233\n",
      "0.1960814597779516\n",
      "tensor([0.3197, 0.3217, 0.3149, 0.3180, 0.3156, 0.3187, 0.3264, 0.3200, 0.3255,\n",
      "        0.3219, 0.3153, 0.3224, 0.3187, 0.3131, 0.3198, 0.3184, 0.3220, 0.3142,\n",
      "        0.3135, 0.3194, 0.3116, 0.3134, 0.3168, 0.3149, 0.3117, 0.3106, 0.3160,\n",
      "        0.3094, 0.3082, 0.3115, 0.3128, 0.3181, 0.3187, 0.3106, 0.3135, 0.3131,\n",
      "        0.3104, 0.3107, 0.3077, 0.3138, 0.3118, 0.3198, 0.3139, 0.3127, 0.3224,\n",
      "        0.3148, 0.3216, 0.3104, 0.3053, 0.3076, 0.3064, 0.3042, 0.3045, 0.3060,\n",
      "        0.3180, 0.3169, 0.3227, 0.3205, 0.3175, 0.3218, 0.3199, 0.3278, 0.3253,\n",
      "        0.3255, 0.3250, 0.3224, 0.3169, 0.3216, 0.3146, 0.3051, 0.3126, 0.3175,\n",
      "        0.3112, 0.3131, 0.3143, 0.3129, 0.3146, 0.3123, 0.3122, 0.3117, 0.3111,\n",
      "        0.3154, 0.3197, 0.3196, 0.3158, 0.3187, 0.3144, 0.3142, 0.3152, 0.3120,\n",
      "        0.3124, 0.3112, 0.3144, 0.3118, 0.3157, 0.3212, 0.3146, 0.3170, 0.3262,\n",
      "        0.3225, 0.3274, 0.3195, 0.3168, 0.3189, 0.3163, 0.3143, 0.3166, 0.3203,\n",
      "        0.3201, 0.3173, 0.3240, 0.3185, 0.3188, 0.3150, 0.3148, 0.3203, 0.3238,\n",
      "        0.3200, 0.3154, 0.3165, 0.3196, 0.3144, 0.3146, 0.3128, 0.3158, 0.3140,\n",
      "        0.3135, 0.3192, 0.3096, 0.3116, 0.3097, 0.3110, 0.3084, 0.3133, 0.3104,\n",
      "        0.3103, 0.3060, 0.3103, 0.3105, 0.3098, 0.3126, 0.3107, 0.3150, 0.3184,\n",
      "        0.3194, 0.3142, 0.3145, 0.3152, 0.3137, 0.3162, 0.3141, 0.3102, 0.3127,\n",
      "        0.3117, 0.3138, 0.3126, 0.3168, 0.3131, 0.3148, 0.3156, 0.3122, 0.3189,\n",
      "        0.3129, 0.3168, 0.3183, 0.3104, 0.3133, 0.3117, 0.3158, 0.3135, 0.3146,\n",
      "        0.3178, 0.3137, 0.3146, 0.3119, 0.3153, 0.3107, 0.3129, 0.3113, 0.3127,\n",
      "        0.3103, 0.3085, 0.3136, 0.3106, 0.3109, 0.3186, 0.3124, 0.3222, 0.3159,\n",
      "        0.3212, 0.3174, 0.3251, 0.3197, 0.3162, 0.3198, 0.3176, 0.3140, 0.3110,\n",
      "        0.3140, 0.3098, 0.3208, 0.3144, 0.3181, 0.3257, 0.3313, 0.3255, 0.3172,\n",
      "        0.3176, 0.3270, 0.3189, 0.3201, 0.3161, 0.3303, 0.3145, 0.3137, 0.3201,\n",
      "        0.3142, 0.3218, 0.3169, 0.3117, 0.3095, 0.3109, 0.3104, 0.3093, 0.3106,\n",
      "        0.3117, 0.3088, 0.3127, 0.3203, 0.3276, 0.3200, 0.3288, 0.3271, 0.3296,\n",
      "        0.3260, 0.3252, 0.3292, 0.3301, 0.3319, 0.3124, 0.3197, 0.3153, 0.3152,\n",
      "        0.3188, 0.3169, 0.3135, 0.3132, 0.3141, 0.3199, 0.3168, 0.3178, 0.3143,\n",
      "        0.3169, 0.3116, 0.3174, 0.3100, 0.3161, 0.3153, 0.3224, 0.3154, 0.3253,\n",
      "        0.3169, 0.3160, 0.3131, 0.3146, 0.3138, 0.3154, 0.3171, 0.3129, 0.3123,\n",
      "        0.3099, 0.3151, 0.3113, 0.3104, 0.3089, 0.3161, 0.3103, 0.3129, 0.3136,\n",
      "        0.3103, 0.3100, 0.3159, 0.3117, 0.3076, 0.3073, 0.3049, 0.3167, 0.3104,\n",
      "        0.3175, 0.3161, 0.3108, 0.3113, 0.3133, 0.3148, 0.3133, 0.3108, 0.3112,\n",
      "        0.3149, 0.3115, 0.3043, 0.3162, 0.3109, 0.3120, 0.3116, 0.3110, 0.3068,\n",
      "        0.3107, 0.3117, 0.3161, 0.3121, 0.3123, 0.3081, 0.3120, 0.3134, 0.3101,\n",
      "        0.3145, 0.3132, 0.3042, 0.3190, 0.3122, 0.3136, 0.3118, 0.3106, 0.3098,\n",
      "        0.3145, 0.3111, 0.3104, 0.3110, 0.3129, 0.3111, 0.3120, 0.3131, 0.3100,\n",
      "        0.3149, 0.3127, 0.3104, 0.3125, 0.3196, 0.3235, 0.3200, 0.3208, 0.3171,\n",
      "        0.3158, 0.3234, 0.3190, 0.3165, 0.3207, 0.3169, 0.3173, 0.3189, 0.3154,\n",
      "        0.3136, 0.3142, 0.3204, 0.3144, 0.3221, 0.3184, 0.3129, 0.3127, 0.3177,\n",
      "        0.3154, 0.3165, 0.3189, 0.3282, 0.3268, 0.3249, 0.3355, 0.3242, 0.3237,\n",
      "        0.3283, 0.3223, 0.3225, 0.3180, 0.3270, 0.3215, 0.3244, 0.3287, 0.3210,\n",
      "        0.3226, 0.3118, 0.3178, 0.3140, 0.3126, 0.3140, 0.3128, 0.3145, 0.3115,\n",
      "        0.3170, 0.3155, 0.3201, 0.3139, 0.3164, 0.3162, 0.3185, 0.3120, 0.3096,\n",
      "        0.3166, 0.3181, 0.3126, 0.3150, 0.3125, 0.3130, 0.3100, 0.3116, 0.3122,\n",
      "        0.3112, 0.3127, 0.3220, 0.3146, 0.3135, 0.3144, 0.3269, 0.3170, 0.3166,\n",
      "        0.3210, 0.3131, 0.3156, 0.3130, 0.3113, 0.3137, 0.3144, 0.3162, 0.3126,\n",
      "        0.3170, 0.3164, 0.3163, 0.3191, 0.3163, 0.3193, 0.3145, 0.3140, 0.3114,\n",
      "        0.3122, 0.3111, 0.3115, 0.3087, 0.3108, 0.3140, 0.3121, 0.3237, 0.3145,\n",
      "        0.3126, 0.3171, 0.3168, 0.3170, 0.3204, 0.3171, 0.3151, 0.3169, 0.3122,\n",
      "        0.3162, 0.3170, 0.3211, 0.3152, 0.3189, 0.3203, 0.3181, 0.3182, 0.3157,\n",
      "        0.3157, 0.3109, 0.3163, 0.3128, 0.3130, 0.3152, 0.3154, 0.3153, 0.3162,\n",
      "        0.3197, 0.3120, 0.3183, 0.3085, 0.3135, 0.3135, 0.3118, 0.3107, 0.3107,\n",
      "        0.3147, 0.3110, 0.3080, 0.3178, 0.3133, 0.3157, 0.3221, 0.3200, 0.3154,\n",
      "        0.3139, 0.3173, 0.3217, 0.3148, 0.3181, 0.3268, 0.3138, 0.3180, 0.3145,\n",
      "        0.3210, 0.3121, 0.3119, 0.3149, 0.3158, 0.3170, 0.3130, 0.3109, 0.3133,\n",
      "        0.3116, 0.3119, 0.3132, 0.3128, 0.3139, 0.3157, 0.3098, 0.3084, 0.3150,\n",
      "        0.3073, 0.3198, 0.3264, 0.3184, 0.3179, 0.3191, 0.3169, 0.3227, 0.3192,\n",
      "        0.3119, 0.3183, 0.3133, 0.3134, 0.3071, 0.3106, 0.3148, 0.3161, 0.3141,\n",
      "        0.3158, 0.3200, 0.3167, 0.3178, 0.3142, 0.3160, 0.3150, 0.3131, 0.3210,\n",
      "        0.3040, 0.3155, 0.3154, 0.3133, 0.3115, 0.3074, 0.3091, 0.3103, 0.3137,\n",
      "        0.3144, 0.3185, 0.3146, 0.3139, 0.3161, 0.3156, 0.3220, 0.3168, 0.3132,\n",
      "        0.3155, 0.3170, 0.3134, 0.3127, 0.3104, 0.3148, 0.3077, 0.3109, 0.3135,\n",
      "        0.3159, 0.3155, 0.3242, 0.3211, 0.3173, 0.3159, 0.3166, 0.3161, 0.3099,\n",
      "        0.3117, 0.3207, 0.3108, 0.3090, 0.3091, 0.3097, 0.3036, 0.3136, 0.3183,\n",
      "        0.3160, 0.3184, 0.3182, 0.3166, 0.3158, 0.3199, 0.3156, 0.3113, 0.3121,\n",
      "        0.3098, 0.3162, 0.3135, 0.3116, 0.3092, 0.3098, 0.3109, 0.3165, 0.3088,\n",
      "        0.3103, 0.3139, 0.3133, 0.3101, 0.3097, 0.3113, 0.3164, 0.3136, 0.3099,\n",
      "        0.3084, 0.3108, 0.3183, 0.3133, 0.3132, 0.3151, 0.3134, 0.3136, 0.3175,\n",
      "        0.3128, 0.3130, 0.3118, 0.3138, 0.3132, 0.3127, 0.3090, 0.3109, 0.3071,\n",
      "        0.3129, 0.3046, 0.3176, 0.3101, 0.3139, 0.3105, 0.3115, 0.3128, 0.3103,\n",
      "        0.3139, 0.3101, 0.3091, 0.3176, 0.3129, 0.3112, 0.3128, 0.3193, 0.3112,\n",
      "        0.3126, 0.3149, 0.3170, 0.3140, 0.3134, 0.3120, 0.3147, 0.3157, 0.3141,\n",
      "        0.3156, 0.3193, 0.3145, 0.3188, 0.3165, 0.3132, 0.3136, 0.3123, 0.3137,\n",
      "        0.3157, 0.3200, 0.3182, 0.3217, 0.3168, 0.3163, 0.3148, 0.3195, 0.3152,\n",
      "        0.3150, 0.3140, 0.3138, 0.3187, 0.3187, 0.3148, 0.3113, 0.3101, 0.3156,\n",
      "        0.3138, 0.3130, 0.3149, 0.3159, 0.3118, 0.3047, 0.3106, 0.3105, 0.3118,\n",
      "        0.3118, 0.3117, 0.3123, 0.3139, 0.3107, 0.3136, 0.3162, 0.3156, 0.3194,\n",
      "        0.3174, 0.3126, 0.3170, 0.3139, 0.3149, 0.3141, 0.3141, 0.3118, 0.3105,\n",
      "        0.3098, 0.3104, 0.3096, 0.3069, 0.3151, 0.3122, 0.3066, 0.3066, 0.3071,\n",
      "        0.3117, 0.3092, 0.3079, 0.3118, 0.3224, 0.3159, 0.3105, 0.3122, 0.3125,\n",
      "        0.3129, 0.3102, 0.3072, 0.3150, 0.3111, 0.3114, 0.3142, 0.3124, 0.3058,\n",
      "        0.3124, 0.3182, 0.3119, 0.3151, 0.3135, 0.3177, 0.3159, 0.3158, 0.3118,\n",
      "        0.3190, 0.3160, 0.3225, 0.3142, 0.3205, 0.3138, 0.3154, 0.3143, 0.3197,\n",
      "        0.3157, 0.3150, 0.3131, 0.3139, 0.3174, 0.3325, 0.3266, 0.3245, 0.3173,\n",
      "        0.3209, 0.3196, 0.3187, 0.3245, 0.3211, 0.3171, 0.3175, 0.3170, 0.3185,\n",
      "        0.3178, 0.3163, 0.3192, 0.3233, 0.3144, 0.3109, 0.3092, 0.3088, 0.3144,\n",
      "        0.3131, 0.3172, 0.3098, 0.3157, 0.3103, 0.3106, 0.3109, 0.3152, 0.3123,\n",
      "        0.3216, 0.3167, 0.3069, 0.3127, 0.3187, 0.3133, 0.3120, 0.3125, 0.3108,\n",
      "        0.3166, 0.3113, 0.3098, 0.3134, 0.3158, 0.3111, 0.3168, 0.3135, 0.3086,\n",
      "        0.3140, 0.3141, 0.3140, 0.3108, 0.3120, 0.3164, 0.3127, 0.3014, 0.3121,\n",
      "        0.3066, 0.3075, 0.3082, 0.3074, 0.3096, 0.3067, 0.3119, 0.3182, 0.3166,\n",
      "        0.3173, 0.3211, 0.3174, 0.3255, 0.3202, 0.3184, 0.3198, 0.3258, 0.3101,\n",
      "        0.3101, 0.3159, 0.3122, 0.3155, 0.3126, 0.3107, 0.3140, 0.3092, 0.3177,\n",
      "        0.3132, 0.3047, 0.3150, 0.3217, 0.3133, 0.3191, 0.3177, 0.3124, 0.3148,\n",
      "        0.3164, 0.3207, 0.3144, 0.3281, 0.3128, 0.3184, 0.3234, 0.3151, 0.3135,\n",
      "        0.3120, 0.3133, 0.3094, 0.3131, 0.3156, 0.3169, 0.3100, 0.3190, 0.3101,\n",
      "        0.3132, 0.3120, 0.3153, 0.3127, 0.3102, 0.3105, 0.3109, 0.3125, 0.3140,\n",
      "        0.3144, 0.3177, 0.3113, 0.3160, 0.3118, 0.3131, 0.3190, 0.3146, 0.3151,\n",
      "        0.3153, 0.3140, 0.3235, 0.3167, 0.3125, 0.3128, 0.3114, 0.3137, 0.3146,\n",
      "        0.3270, 0.3148, 0.3195, 0.3265, 0.3211, 0.3234, 0.3125, 0.3132, 0.3112,\n",
      "        0.3119, 0.3171, 0.3166, 0.3122, 0.3104, 0.3154, 0.3193, 0.3173, 0.3196,\n",
      "        0.3090, 0.3158, 0.3198, 0.3205, 0.3189, 0.3206, 0.3233, 0.3166, 0.3205,\n",
      "        0.3192, 0.3157, 0.3142, 0.3125, 0.3122, 0.3126, 0.3127, 0.3130, 0.3166,\n",
      "        0.3130, 0.3130, 0.3157, 0.3151, 0.3146, 0.3215, 0.3196, 0.3139, 0.3130,\n",
      "        0.3145, 0.3213, 0.3139, 0.3162, 0.3055, 0.3065, 0.3056, 0.3088, 0.3077,\n",
      "        0.3077, 0.3061, 0.3047, 0.3046, 0.3030, 0.3146, 0.3217, 0.3150, 0.3141,\n",
      "        0.3138, 0.3169], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "l1_error = 0\n",
    "l2_error = 0\n",
    "edge_probs = model(data)\n",
    "\n",
    "for i, e in enumerate(G.edges()):\n",
    "  u, v = e\n",
    "  p = G[u][v]['weight']\n",
    "  l1_error += abs(p - edge_probs[i].item())\n",
    "  l2_error += (p - edge_probs[i].item())**2\n",
    "\n",
    "print(l1_error)\n",
    "print(l1_error / G.number_of_edges())\n",
    "print(edge_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Total Loss: 13271.6597\n",
      "Epoch 11/100, Total Loss: 10534.9819\n",
      "Epoch 21/100, Total Loss: 9552.8940\n",
      "Epoch 31/100, Total Loss: 9453.4146\n",
      "Epoch 41/100, Total Loss: 9430.0796\n",
      "Epoch 51/100, Total Loss: 9422.7651\n",
      "Epoch 61/100, Total Loss: 9424.0000\n",
      "Epoch 71/100, Total Loss: 9418.0762\n",
      "Epoch 81/100, Total Loss: 9417.8579\n",
      "Epoch 91/100, Total Loss: 9419.9067\n",
      "Epoch 100/100, Total Loss: 9419.1880\n"
     ]
    }
   ],
   "source": [
    "class GNNIndependentCascade(torch.nn.Module):\n",
    "  def __init__(self, in_dim, hidden_dim, n_nodes, n_edges, num_layers=2):\n",
    "    super(GNNIndependentCascade, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.node_embed = nn.Embedding(n_nodes, in_dim)\n",
    "    self.edge_embed = nn.Parameter(torch.rand((n_edges, hidden_dim)))\n",
    "\n",
    "    self.convs = nn.ModuleList([\n",
    "      SAGEConv(in_dim if i == 0 else hidden_dim, hidden_dim) \n",
    "      for i in range(num_layers)\n",
    "    ])\n",
    "\n",
    "    self.edge_predictor = nn.Sequential(\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ELU(),\n",
    "      nn.Linear(hidden_dim, 1),\n",
    "    )\n",
    "    #nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, data):\n",
    "    x, edge_index = data.x, data.edge_index\n",
    "\n",
    "    # Node embedding\n",
    "    #for i in range(self.num_layers):\n",
    "    #  x = self.convs[i](x, edge_index)\n",
    "    #  x = torch.relu(x)\n",
    "    #  x = torch.dropout(x, p=0.1, train=self.training)\n",
    "\n",
    "    # Edge probability prediction\n",
    "    edge_probs = torch.sigmoid(self.edge_predictor(self.edge_embed))\n",
    "\n",
    "    #edge_probs = torch.sigmoid(torch.sum(x[row] * x[col], dim=1))\n",
    "\n",
    "    return edge_probs\n",
    "\n",
    "def train_model(model, optimizer, data, cascades, num_epochs, batch_size = 50):\n",
    "  model.train()\n",
    "  #batches = DataLoader(cascades, batch_size=10, shuffle=True)\n",
    "  #print(batches)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    loss = 0.0\n",
    "    rng.shuffle(cascades)\n",
    "    batches = [cascades[i:i+batch_size] for i in range(0, len(cascades), batch_size)]\n",
    "\n",
    "    for batch in batches:\n",
    "      optimizer.zero_grad()\n",
    "      edge_probs = model.forward(data)\n",
    "\n",
    "      batch_loss = compute_loss(edge_probs, data.edge_index, batch)\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      loss += batch_loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {loss:.4f}\")\n",
    "      #print(edge_probs[0])\n",
    "      #print(edge_probs[1])\n",
    "      #print('\\n')\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "features_embed = nn.Embedding(n, n)\n",
    "data = create_dataset(G, features_embed(torch.arange(n)))\n",
    "model = GNNIndependentCascade(data.num_features, 64, len(data.x), len(data.edge_index[0]), num_layers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "train_model(model, optimizer, data, cascades, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.2113658612902\n",
      "0.0986646278355339\n",
      "tensor([[6.1738e-02],\n",
      "        [7.9006e-01],\n",
      "        [4.0442e-01],\n",
      "        [4.4102e-01],\n",
      "        [8.2723e-01],\n",
      "        [6.6342e-01],\n",
      "        [3.3259e-01],\n",
      "        [4.0768e-01],\n",
      "        [1.7554e-01],\n",
      "        [5.3576e-01],\n",
      "        [5.7342e-01],\n",
      "        [2.8838e-01],\n",
      "        [2.6197e-01],\n",
      "        [9.4849e-02],\n",
      "        [2.8793e-01],\n",
      "        [2.8896e-02],\n",
      "        [2.3368e-01],\n",
      "        [3.3511e-01],\n",
      "        [1.2920e-03],\n",
      "        [4.6355e-01],\n",
      "        [2.6946e-01],\n",
      "        [3.1625e-01],\n",
      "        [1.6136e-01],\n",
      "        [8.2319e-02],\n",
      "        [7.2024e-01],\n",
      "        [1.6765e-01],\n",
      "        [1.4062e-01],\n",
      "        [2.5346e-01],\n",
      "        [1.4348e-01],\n",
      "        [1.7394e-01],\n",
      "        [5.1224e-01],\n",
      "        [5.3003e-01],\n",
      "        [7.0310e-02],\n",
      "        [6.7588e-01],\n",
      "        [4.4773e-01],\n",
      "        [4.0013e-01],\n",
      "        [6.8070e-02],\n",
      "        [2.6520e-01],\n",
      "        [1.3172e-03],\n",
      "        [4.7475e-01],\n",
      "        [3.6202e-01],\n",
      "        [4.5034e-01],\n",
      "        [5.1787e-01],\n",
      "        [3.3955e-01],\n",
      "        [8.8832e-01],\n",
      "        [1.0132e-01],\n",
      "        [4.4119e-01],\n",
      "        [5.8549e-03],\n",
      "        [5.2611e-01],\n",
      "        [1.4500e-03],\n",
      "        [3.7825e-01],\n",
      "        [1.0237e-01],\n",
      "        [4.8658e-01],\n",
      "        [3.2036e-01],\n",
      "        [7.8922e-01],\n",
      "        [1.0156e-01],\n",
      "        [1.8758e-01],\n",
      "        [6.4280e-01],\n",
      "        [9.1774e-01],\n",
      "        [9.9665e-01],\n",
      "        [3.8158e-01],\n",
      "        [2.3038e-01],\n",
      "        [4.7884e-01],\n",
      "        [2.5789e-01],\n",
      "        [7.0945e-01],\n",
      "        [2.8473e-01],\n",
      "        [1.9138e-03],\n",
      "        [4.9013e-01],\n",
      "        [2.2289e-01],\n",
      "        [6.1186e-01],\n",
      "        [1.1211e-03],\n",
      "        [1.7893e-01],\n",
      "        [5.4363e-01],\n",
      "        [7.2260e-01],\n",
      "        [1.1475e-01],\n",
      "        [1.2506e-01],\n",
      "        [5.2886e-01],\n",
      "        [5.0822e-02],\n",
      "        [4.4251e-01],\n",
      "        [2.8359e-01],\n",
      "        [7.7877e-02],\n",
      "        [3.3889e-01],\n",
      "        [8.5619e-01],\n",
      "        [3.3697e-01],\n",
      "        [6.7013e-01],\n",
      "        [1.7025e-03],\n",
      "        [2.2336e-01],\n",
      "        [6.5819e-03],\n",
      "        [4.4302e-01],\n",
      "        [4.3332e-01],\n",
      "        [1.9470e-03],\n",
      "        [4.0691e-01],\n",
      "        [3.7656e-01],\n",
      "        [3.6337e-01],\n",
      "        [3.9616e-01],\n",
      "        [7.7426e-01],\n",
      "        [3.5427e-01],\n",
      "        [1.6028e-01],\n",
      "        [4.4753e-01],\n",
      "        [2.3701e-01],\n",
      "        [1.1763e-03],\n",
      "        [3.9740e-01],\n",
      "        [5.4908e-01],\n",
      "        [7.0740e-01],\n",
      "        [2.3280e-01],\n",
      "        [7.5624e-01],\n",
      "        [5.0515e-01],\n",
      "        [1.1642e-01],\n",
      "        [2.3384e-01],\n",
      "        [9.3485e-04],\n",
      "        [1.8990e-01],\n",
      "        [1.8357e-01],\n",
      "        [1.6061e-03],\n",
      "        [4.7680e-01],\n",
      "        [1.9034e-01],\n",
      "        [4.3635e-01],\n",
      "        [3.9016e-01],\n",
      "        [1.8341e-01],\n",
      "        [7.7732e-01],\n",
      "        [2.5417e-01],\n",
      "        [5.5707e-01],\n",
      "        [1.0303e-01],\n",
      "        [4.3181e-01],\n",
      "        [3.5238e-01],\n",
      "        [4.2694e-01],\n",
      "        [2.1303e-01],\n",
      "        [2.4612e-03],\n",
      "        [5.6481e-01],\n",
      "        [3.6970e-01],\n",
      "        [5.5092e-01],\n",
      "        [2.6468e-01],\n",
      "        [7.8607e-01],\n",
      "        [1.5979e-01],\n",
      "        [3.8450e-01],\n",
      "        [4.0396e-01],\n",
      "        [5.2576e-01],\n",
      "        [1.1640e-01],\n",
      "        [1.6777e-03],\n",
      "        [1.6564e-03],\n",
      "        [1.6764e-01],\n",
      "        [1.2867e-03],\n",
      "        [2.3260e-03],\n",
      "        [1.2656e-01],\n",
      "        [9.0196e-01],\n",
      "        [7.6447e-01],\n",
      "        [5.7496e-01],\n",
      "        [5.1958e-01],\n",
      "        [1.1754e-03],\n",
      "        [8.0121e-02],\n",
      "        [3.7605e-01],\n",
      "        [1.9100e-03],\n",
      "        [4.9042e-01],\n",
      "        [1.7302e-01],\n",
      "        [1.2158e-03],\n",
      "        [5.8709e-01],\n",
      "        [7.2195e-04],\n",
      "        [2.1241e-01],\n",
      "        [7.8644e-01],\n",
      "        [1.7803e-01],\n",
      "        [4.6742e-01],\n",
      "        [4.1394e-01],\n",
      "        [3.5584e-01],\n",
      "        [1.6423e-01],\n",
      "        [3.6176e-01],\n",
      "        [7.6536e-01],\n",
      "        [3.2417e-02],\n",
      "        [9.3427e-04],\n",
      "        [4.4035e-01],\n",
      "        [4.3655e-01],\n",
      "        [1.6146e-01],\n",
      "        [9.0097e-01],\n",
      "        [3.7259e-01],\n",
      "        [8.1990e-01],\n",
      "        [1.1577e-03],\n",
      "        [8.2126e-04],\n",
      "        [4.9755e-01],\n",
      "        [1.2773e-01],\n",
      "        [1.0073e-01],\n",
      "        [3.9652e-01],\n",
      "        [6.9909e-02],\n",
      "        [1.6610e-03],\n",
      "        [8.6967e-04],\n",
      "        [1.0950e-01],\n",
      "        [3.1935e-01],\n",
      "        [4.6705e-01],\n",
      "        [1.2783e-03],\n",
      "        [6.9745e-04],\n",
      "        [4.7092e-01],\n",
      "        [2.8609e-01],\n",
      "        [4.3422e-01],\n",
      "        [2.6468e-03],\n",
      "        [4.7039e-01],\n",
      "        [2.9255e-01],\n",
      "        [7.6957e-04],\n",
      "        [5.5393e-01],\n",
      "        [3.2511e-01],\n",
      "        [5.5638e-01],\n",
      "        [1.1483e-01],\n",
      "        [3.8997e-01],\n",
      "        [5.1675e-01],\n",
      "        [1.3916e-03],\n",
      "        [4.2495e-03],\n",
      "        [6.3160e-01],\n",
      "        [4.7939e-01],\n",
      "        [9.9519e-01],\n",
      "        [8.2081e-04],\n",
      "        [3.4765e-01],\n",
      "        [6.3374e-01],\n",
      "        [4.0722e-01],\n",
      "        [4.5058e-01],\n",
      "        [2.0715e-01],\n",
      "        [3.4358e-01],\n",
      "        [2.9300e-01],\n",
      "        [7.9521e-04],\n",
      "        [7.6598e-01],\n",
      "        [2.2318e-01],\n",
      "        [1.2062e-01],\n",
      "        [5.6759e-01],\n",
      "        [5.1238e-01],\n",
      "        [2.8245e-01],\n",
      "        [5.1510e-01],\n",
      "        [3.6941e-01],\n",
      "        [2.4948e-01],\n",
      "        [9.4671e-04],\n",
      "        [5.9495e-04],\n",
      "        [2.7392e-01],\n",
      "        [5.5260e-01],\n",
      "        [1.1742e-03],\n",
      "        [3.2474e-01],\n",
      "        [5.2337e-01],\n",
      "        [4.1413e-01],\n",
      "        [2.8844e-01],\n",
      "        [6.7860e-01],\n",
      "        [2.1576e-01],\n",
      "        [8.1187e-04],\n",
      "        [2.9630e-03],\n",
      "        [5.4972e-01],\n",
      "        [5.9557e-02],\n",
      "        [5.9731e-01],\n",
      "        [1.3759e-01],\n",
      "        [4.0155e-01],\n",
      "        [2.0047e-01],\n",
      "        [1.2213e-03],\n",
      "        [6.3582e-01],\n",
      "        [4.6980e-01],\n",
      "        [2.1949e-01],\n",
      "        [9.7708e-04],\n",
      "        [2.8509e-01],\n",
      "        [5.3573e-01],\n",
      "        [6.0500e-03],\n",
      "        [3.1931e-01],\n",
      "        [4.2967e-01],\n",
      "        [5.3359e-01],\n",
      "        [9.4561e-02],\n",
      "        [3.1489e-01],\n",
      "        [4.9970e-01],\n",
      "        [6.2689e-01],\n",
      "        [2.6274e-01],\n",
      "        [1.7124e-03],\n",
      "        [2.9221e-01],\n",
      "        [3.0677e-01],\n",
      "        [1.2470e-03],\n",
      "        [5.1120e-01],\n",
      "        [9.6922e-01],\n",
      "        [4.1999e-03],\n",
      "        [4.6375e-01],\n",
      "        [2.7548e-01],\n",
      "        [3.0636e-01],\n",
      "        [1.4074e-03],\n",
      "        [4.3910e-03],\n",
      "        [7.1805e-02],\n",
      "        [5.1446e-01],\n",
      "        [9.9992e-04],\n",
      "        [9.5739e-04],\n",
      "        [2.3254e-01],\n",
      "        [2.6576e-01],\n",
      "        [6.4956e-01],\n",
      "        [4.7841e-01],\n",
      "        [2.3795e-01],\n",
      "        [2.9281e-01],\n",
      "        [3.8739e-01],\n",
      "        [5.3428e-01],\n",
      "        [9.7978e-04],\n",
      "        [1.0622e-01],\n",
      "        [2.6745e-01],\n",
      "        [1.9020e-01],\n",
      "        [8.1180e-01],\n",
      "        [2.3239e-01],\n",
      "        [1.5377e-01],\n",
      "        [1.0315e-03],\n",
      "        [1.6471e-01],\n",
      "        [1.2775e-03],\n",
      "        [1.7046e-01],\n",
      "        [6.8594e-01],\n",
      "        [1.1117e-03],\n",
      "        [1.6022e-03],\n",
      "        [4.9075e-01],\n",
      "        [6.4862e-01],\n",
      "        [2.8648e-01],\n",
      "        [4.1501e-01],\n",
      "        [2.0927e-01],\n",
      "        [6.5642e-01],\n",
      "        [4.2750e-01],\n",
      "        [3.0184e-01],\n",
      "        [1.1923e-02],\n",
      "        [2.0315e-03],\n",
      "        [5.6395e-01],\n",
      "        [4.6056e-01],\n",
      "        [4.6926e-01],\n",
      "        [6.9240e-01],\n",
      "        [6.3295e-01],\n",
      "        [7.4843e-01],\n",
      "        [4.8467e-01],\n",
      "        [1.1069e-03],\n",
      "        [2.1168e-01],\n",
      "        [1.1281e-03],\n",
      "        [1.2824e-01],\n",
      "        [2.0734e-01],\n",
      "        [8.9209e-02],\n",
      "        [2.2742e-01],\n",
      "        [6.0373e-01],\n",
      "        [6.9096e-01],\n",
      "        [3.4243e-01],\n",
      "        [8.3863e-04],\n",
      "        [9.3173e-04],\n",
      "        [4.0716e-01],\n",
      "        [2.7503e-01],\n",
      "        [6.9889e-01],\n",
      "        [2.3658e-01],\n",
      "        [2.8040e-01],\n",
      "        [1.9874e-01],\n",
      "        [9.6283e-02],\n",
      "        [1.0844e-03],\n",
      "        [2.0443e-01],\n",
      "        [4.6712e-01],\n",
      "        [8.2128e-01],\n",
      "        [1.2812e-03],\n",
      "        [1.8537e-01],\n",
      "        [7.9071e-02],\n",
      "        [1.4544e-02],\n",
      "        [6.6578e-01],\n",
      "        [7.3929e-01],\n",
      "        [5.4043e-01],\n",
      "        [6.1716e-01],\n",
      "        [2.9011e-01],\n",
      "        [1.5868e-01],\n",
      "        [5.0646e-02],\n",
      "        [5.9540e-01],\n",
      "        [1.2071e-03],\n",
      "        [2.8493e-01],\n",
      "        [3.0177e-01],\n",
      "        [4.6469e-01],\n",
      "        [2.2800e-01],\n",
      "        [1.0790e-01],\n",
      "        [1.9575e-01],\n",
      "        [9.4561e-02],\n",
      "        [6.7985e-01],\n",
      "        [5.0165e-01],\n",
      "        [7.9016e-01],\n",
      "        [6.3022e-01],\n",
      "        [1.3529e-03],\n",
      "        [2.9855e-01],\n",
      "        [8.8987e-04],\n",
      "        [4.9627e-01],\n",
      "        [7.1874e-01],\n",
      "        [4.2899e-01],\n",
      "        [8.3464e-01],\n",
      "        [5.7879e-01],\n",
      "        [3.9426e-01],\n",
      "        [1.2040e-03],\n",
      "        [4.1843e-01],\n",
      "        [9.3299e-01],\n",
      "        [1.5173e-01],\n",
      "        [7.8048e-02],\n",
      "        [4.1909e-01],\n",
      "        [8.9172e-01],\n",
      "        [1.7425e-01],\n",
      "        [6.8929e-02],\n",
      "        [9.4153e-04],\n",
      "        [4.0378e-01],\n",
      "        [1.0248e-03],\n",
      "        [4.3447e-03],\n",
      "        [4.0138e-01],\n",
      "        [9.0990e-01],\n",
      "        [1.2159e-03],\n",
      "        [2.0916e-03],\n",
      "        [4.1705e-01],\n",
      "        [3.8925e-01],\n",
      "        [5.6789e-01],\n",
      "        [6.6359e-04],\n",
      "        [5.1792e-01],\n",
      "        [6.0402e-03],\n",
      "        [5.8315e-01],\n",
      "        [9.0798e-01],\n",
      "        [1.8870e-01],\n",
      "        [1.4995e-03],\n",
      "        [6.9559e-01],\n",
      "        [2.2900e-01],\n",
      "        [2.7725e-03],\n",
      "        [4.1441e-01],\n",
      "        [2.3632e-01],\n",
      "        [2.6302e-01],\n",
      "        [1.9432e-01],\n",
      "        [9.1111e-04],\n",
      "        [6.9057e-01],\n",
      "        [1.0502e-03],\n",
      "        [7.5868e-01],\n",
      "        [8.5073e-01],\n",
      "        [4.8902e-01],\n",
      "        [1.4988e-03],\n",
      "        [2.9795e-01],\n",
      "        [3.9089e-01],\n",
      "        [2.8192e-01],\n",
      "        [1.0229e-03],\n",
      "        [3.6322e-01],\n",
      "        [3.1808e-01],\n",
      "        [1.6622e-01],\n",
      "        [7.8343e-01],\n",
      "        [2.7181e-03],\n",
      "        [1.2825e-03],\n",
      "        [6.3303e-01],\n",
      "        [7.8898e-01],\n",
      "        [5.5733e-01],\n",
      "        [2.8164e-01],\n",
      "        [5.4854e-01],\n",
      "        [2.5499e-01],\n",
      "        [4.6911e-01],\n",
      "        [7.3995e-04],\n",
      "        [1.3971e-03],\n",
      "        [5.8612e-01],\n",
      "        [3.9949e-02],\n",
      "        [2.0790e-01],\n",
      "        [4.4837e-01],\n",
      "        [4.8686e-01],\n",
      "        [7.8738e-04],\n",
      "        [1.7590e-01],\n",
      "        [1.0027e-01],\n",
      "        [2.6097e-01],\n",
      "        [2.6403e-01],\n",
      "        [3.1179e-01],\n",
      "        [7.2861e-04],\n",
      "        [1.0482e-01],\n",
      "        [4.3436e-01],\n",
      "        [2.5601e-01],\n",
      "        [4.3733e-01],\n",
      "        [3.6660e-01],\n",
      "        [6.7803e-01],\n",
      "        [4.2042e-01],\n",
      "        [5.6714e-01],\n",
      "        [3.5296e-01],\n",
      "        [8.0605e-04],\n",
      "        [1.0628e-01],\n",
      "        [1.9838e-01],\n",
      "        [2.4527e-03],\n",
      "        [6.0511e-01],\n",
      "        [8.9345e-04],\n",
      "        [1.2137e-03],\n",
      "        [3.5948e-01],\n",
      "        [3.0507e-01],\n",
      "        [3.1900e-01],\n",
      "        [4.4831e-01],\n",
      "        [6.9848e-01],\n",
      "        [3.4187e-01],\n",
      "        [4.6145e-01],\n",
      "        [1.1910e-01],\n",
      "        [2.0611e-02],\n",
      "        [7.7511e-04],\n",
      "        [6.8958e-01],\n",
      "        [4.9438e-04],\n",
      "        [5.9890e-03],\n",
      "        [1.4091e-01],\n",
      "        [3.5269e-02],\n",
      "        [5.1724e-01],\n",
      "        [9.2751e-04],\n",
      "        [9.9001e-02],\n",
      "        [6.8392e-01],\n",
      "        [6.4731e-03],\n",
      "        [7.0915e-01],\n",
      "        [1.2476e-01],\n",
      "        [8.1186e-02],\n",
      "        [1.2128e-01],\n",
      "        [4.1927e-01],\n",
      "        [5.9468e-01],\n",
      "        [4.8734e-01],\n",
      "        [4.9142e-01],\n",
      "        [1.9410e-01],\n",
      "        [5.0864e-01],\n",
      "        [7.7199e-02],\n",
      "        [6.0895e-04],\n",
      "        [9.9598e-01],\n",
      "        [3.3278e-01],\n",
      "        [1.5414e-01],\n",
      "        [1.0280e-01],\n",
      "        [3.6367e-01],\n",
      "        [4.7437e-01],\n",
      "        [4.8174e-01],\n",
      "        [6.6838e-04],\n",
      "        [1.3137e-03],\n",
      "        [7.9968e-04],\n",
      "        [9.9437e-01],\n",
      "        [4.9366e-01],\n",
      "        [1.4796e-01],\n",
      "        [4.1839e-01],\n",
      "        [2.0442e-03],\n",
      "        [3.2847e-01],\n",
      "        [2.1991e-01],\n",
      "        [4.6705e-01],\n",
      "        [1.3945e-01],\n",
      "        [1.3335e-03],\n",
      "        [4.9702e-01],\n",
      "        [1.9681e-01],\n",
      "        [2.1345e-01],\n",
      "        [1.6198e-03],\n",
      "        [3.4443e-01],\n",
      "        [1.6041e-03],\n",
      "        [3.4924e-01],\n",
      "        [7.3569e-01],\n",
      "        [6.5379e-01],\n",
      "        [1.0448e-03],\n",
      "        [3.6743e-01],\n",
      "        [7.2338e-02],\n",
      "        [6.2579e-01],\n",
      "        [5.2510e-01],\n",
      "        [5.0304e-01],\n",
      "        [3.2724e-01],\n",
      "        [1.7046e-01],\n",
      "        [6.8249e-01],\n",
      "        [8.0507e-04],\n",
      "        [1.2054e-03],\n",
      "        [5.3273e-01],\n",
      "        [4.0917e-01],\n",
      "        [8.0480e-01],\n",
      "        [4.7100e-01],\n",
      "        [4.5363e-01],\n",
      "        [1.1715e-03],\n",
      "        [3.8309e-01],\n",
      "        [6.5506e-01],\n",
      "        [2.9873e-01],\n",
      "        [4.1272e-01],\n",
      "        [5.2632e-01],\n",
      "        [2.7079e-01],\n",
      "        [4.6030e-01],\n",
      "        [6.1382e-01],\n",
      "        [1.6126e-01],\n",
      "        [6.4568e-01],\n",
      "        [2.3923e-01],\n",
      "        [3.7253e-01],\n",
      "        [3.8839e-01],\n",
      "        [4.2259e-01],\n",
      "        [4.1882e-01],\n",
      "        [7.5581e-01],\n",
      "        [3.8519e-01],\n",
      "        [1.2266e-01],\n",
      "        [2.1629e-01],\n",
      "        [1.6027e-01],\n",
      "        [4.0606e-01],\n",
      "        [7.9459e-01],\n",
      "        [9.5150e-04],\n",
      "        [1.2848e-03],\n",
      "        [1.7936e-01],\n",
      "        [3.1880e-01],\n",
      "        [2.4860e-01],\n",
      "        [3.5689e-01],\n",
      "        [5.4873e-01],\n",
      "        [5.7338e-01],\n",
      "        [7.0157e-04],\n",
      "        [7.4278e-02],\n",
      "        [2.4887e-01],\n",
      "        [2.3519e-01],\n",
      "        [3.5984e-01],\n",
      "        [2.6698e-03],\n",
      "        [5.4769e-01],\n",
      "        [1.5108e-01],\n",
      "        [4.8350e-01],\n",
      "        [1.4538e-03],\n",
      "        [2.0836e-03],\n",
      "        [5.1791e-01],\n",
      "        [3.2285e-01],\n",
      "        [4.6070e-01],\n",
      "        [2.0359e-03],\n",
      "        [2.5425e-01],\n",
      "        [2.7918e-01],\n",
      "        [3.2369e-01],\n",
      "        [3.4428e-02],\n",
      "        [8.1431e-01],\n",
      "        [2.7520e-03],\n",
      "        [6.0997e-01],\n",
      "        [5.5222e-01],\n",
      "        [3.6123e-01],\n",
      "        [8.7152e-04],\n",
      "        [2.9081e-01],\n",
      "        [3.2492e-01],\n",
      "        [5.8113e-01],\n",
      "        [8.2074e-01],\n",
      "        [5.2583e-01],\n",
      "        [5.5813e-01],\n",
      "        [9.8040e-03],\n",
      "        [2.7705e-01],\n",
      "        [1.3885e-01],\n",
      "        [2.7221e-01],\n",
      "        [1.1323e-03],\n",
      "        [1.4096e-01],\n",
      "        [1.7024e-01],\n",
      "        [1.0640e-03],\n",
      "        [9.9500e-01],\n",
      "        [8.3950e-01],\n",
      "        [4.5165e-03],\n",
      "        [9.7951e-02],\n",
      "        [3.8948e-01],\n",
      "        [1.8086e-03],\n",
      "        [8.8220e-04],\n",
      "        [7.1765e-02],\n",
      "        [5.2357e-01],\n",
      "        [3.3940e-01],\n",
      "        [7.0853e-04],\n",
      "        [4.5069e-01],\n",
      "        [3.3033e-01],\n",
      "        [3.6871e-01],\n",
      "        [1.6760e-01],\n",
      "        [1.3232e-01],\n",
      "        [3.5472e-01],\n",
      "        [4.0959e-01],\n",
      "        [1.8904e-03],\n",
      "        [9.4592e-03],\n",
      "        [5.8594e-01],\n",
      "        [2.4001e-01],\n",
      "        [8.6055e-01],\n",
      "        [1.6424e-03],\n",
      "        [4.8245e-01],\n",
      "        [8.1495e-01],\n",
      "        [8.4878e-04],\n",
      "        [1.8954e-01],\n",
      "        [9.3738e-04],\n",
      "        [5.4819e-01],\n",
      "        [4.1156e-01],\n",
      "        [3.5110e-01],\n",
      "        [2.3775e-01],\n",
      "        [6.1281e-01],\n",
      "        [4.0926e-01],\n",
      "        [6.9568e-01],\n",
      "        [7.6376e-02],\n",
      "        [3.6799e-01],\n",
      "        [3.4304e-01],\n",
      "        [3.1533e-03],\n",
      "        [3.5707e-01],\n",
      "        [2.8756e-03],\n",
      "        [1.1049e-01],\n",
      "        [5.1772e-01],\n",
      "        [8.8347e-04],\n",
      "        [4.6494e-01],\n",
      "        [1.2684e-01],\n",
      "        [1.7770e-01],\n",
      "        [2.6225e-01],\n",
      "        [6.1955e-01],\n",
      "        [4.0838e-01],\n",
      "        [7.9141e-04],\n",
      "        [9.5803e-04],\n",
      "        [2.9220e-01],\n",
      "        [4.9751e-01],\n",
      "        [1.1162e-03],\n",
      "        [3.8648e-01],\n",
      "        [1.8415e-03],\n",
      "        [1.1540e-01],\n",
      "        [2.0856e-01],\n",
      "        [5.7917e-01],\n",
      "        [1.9806e-01],\n",
      "        [2.1553e-02],\n",
      "        [1.1164e-01],\n",
      "        [5.9201e-01],\n",
      "        [2.4165e-01],\n",
      "        [4.7059e-01],\n",
      "        [3.9884e-01],\n",
      "        [2.5299e-01],\n",
      "        [6.0101e-01],\n",
      "        [6.3424e-01],\n",
      "        [1.7611e-03],\n",
      "        [1.1739e-02],\n",
      "        [1.8129e-01],\n",
      "        [2.7978e-01],\n",
      "        [1.4527e-03],\n",
      "        [4.2240e-01],\n",
      "        [1.6707e-01],\n",
      "        [2.6872e-01],\n",
      "        [8.0259e-01],\n",
      "        [4.4315e-01],\n",
      "        [1.4634e-02],\n",
      "        [2.0012e-01],\n",
      "        [1.6465e-03],\n",
      "        [3.2376e-01],\n",
      "        [1.3431e-03],\n",
      "        [1.0193e-03],\n",
      "        [9.8694e-04],\n",
      "        [4.4327e-01],\n",
      "        [2.9662e-03],\n",
      "        [2.9260e-01],\n",
      "        [1.0253e-01],\n",
      "        [4.0901e-01],\n",
      "        [3.5805e-01],\n",
      "        [6.4618e-01],\n",
      "        [2.7496e-01],\n",
      "        [5.4432e-01],\n",
      "        [4.7690e-01],\n",
      "        [5.5266e-01],\n",
      "        [8.0983e-02],\n",
      "        [3.2394e-01],\n",
      "        [4.7781e-01],\n",
      "        [1.6727e-01],\n",
      "        [1.8529e-01],\n",
      "        [7.2903e-01],\n",
      "        [6.6962e-01],\n",
      "        [9.9466e-01],\n",
      "        [6.2069e-01],\n",
      "        [7.9096e-01],\n",
      "        [1.6885e-01],\n",
      "        [4.5835e-01],\n",
      "        [2.0167e-01],\n",
      "        [1.7692e-01],\n",
      "        [1.0235e-03],\n",
      "        [3.0054e-01],\n",
      "        [4.9319e-01],\n",
      "        [6.7122e-04],\n",
      "        [1.8372e-01],\n",
      "        [4.6436e-01],\n",
      "        [4.8445e-01],\n",
      "        [5.0793e-01],\n",
      "        [6.7512e-01],\n",
      "        [7.3568e-01],\n",
      "        [1.4400e-01],\n",
      "        [3.9911e-01],\n",
      "        [4.0888e-01],\n",
      "        [1.2387e-03],\n",
      "        [7.0328e-04],\n",
      "        [6.7671e-01],\n",
      "        [8.0149e-01],\n",
      "        [1.8100e-01],\n",
      "        [4.5696e-01],\n",
      "        [2.4088e-01],\n",
      "        [1.1260e-03],\n",
      "        [2.3066e-01],\n",
      "        [7.2048e-01],\n",
      "        [4.0971e-01],\n",
      "        [3.0013e-01],\n",
      "        [3.9266e-01],\n",
      "        [8.5370e-04],\n",
      "        [1.7890e-01],\n",
      "        [3.2490e-01],\n",
      "        [1.0966e-01],\n",
      "        [1.6223e-01],\n",
      "        [3.9332e-01],\n",
      "        [1.8554e-02],\n",
      "        [8.6801e-02],\n",
      "        [1.9257e-03],\n",
      "        [4.7232e-01],\n",
      "        [4.4933e-01],\n",
      "        [4.4695e-01],\n",
      "        [1.2853e-01],\n",
      "        [2.0154e-01],\n",
      "        [1.7004e-01],\n",
      "        [2.2091e-03],\n",
      "        [6.0667e-02],\n",
      "        [7.5918e-01],\n",
      "        [4.2903e-01],\n",
      "        [1.4700e-01],\n",
      "        [3.9909e-01],\n",
      "        [5.7216e-01],\n",
      "        [1.0076e-03],\n",
      "        [1.2721e-03],\n",
      "        [1.9355e-01],\n",
      "        [4.7442e-01],\n",
      "        [1.8792e-01],\n",
      "        [5.3324e-02],\n",
      "        [2.2634e-01],\n",
      "        [7.9139e-02],\n",
      "        [5.0847e-01],\n",
      "        [9.2953e-04],\n",
      "        [1.8366e-01],\n",
      "        [1.0687e-03],\n",
      "        [1.2493e-01],\n",
      "        [4.2782e-01],\n",
      "        [1.1137e-01],\n",
      "        [2.3684e-01],\n",
      "        [4.1155e-01],\n",
      "        [9.0946e-02],\n",
      "        [3.0168e-01],\n",
      "        [2.3304e-02],\n",
      "        [9.7655e-04],\n",
      "        [9.9418e-02],\n",
      "        [5.7364e-01],\n",
      "        [8.4099e-01],\n",
      "        [3.7342e-01],\n",
      "        [1.8460e-03],\n",
      "        [8.0081e-01],\n",
      "        [6.0841e-01],\n",
      "        [7.1631e-01],\n",
      "        [4.6070e-01],\n",
      "        [1.0990e-03],\n",
      "        [2.1790e-01],\n",
      "        [1.1096e-03],\n",
      "        [9.2208e-04],\n",
      "        [3.4653e-01],\n",
      "        [6.3714e-01],\n",
      "        [4.9783e-01],\n",
      "        [5.4560e-01],\n",
      "        [7.0739e-01],\n",
      "        [4.6776e-01],\n",
      "        [5.2156e-01],\n",
      "        [1.0662e-03],\n",
      "        [4.3758e-01],\n",
      "        [9.3465e-02],\n",
      "        [2.9772e-03],\n",
      "        [2.5122e-01],\n",
      "        [6.2660e-02],\n",
      "        [1.8621e-01],\n",
      "        [2.5416e-01],\n",
      "        [1.2669e-03],\n",
      "        [2.4596e-01],\n",
      "        [1.8869e-01],\n",
      "        [3.7522e-01],\n",
      "        [5.3470e-01],\n",
      "        [1.7908e-03],\n",
      "        [5.7512e-01],\n",
      "        [8.2129e-02],\n",
      "        [4.5313e-01],\n",
      "        [7.5044e-04],\n",
      "        [2.2424e-01],\n",
      "        [3.3264e-01],\n",
      "        [3.9248e-01],\n",
      "        [6.4590e-01],\n",
      "        [6.3037e-01],\n",
      "        [2.1756e-01],\n",
      "        [1.4394e-01],\n",
      "        [5.2265e-01],\n",
      "        [4.1502e-01],\n",
      "        [4.2569e-01],\n",
      "        [7.3239e-01],\n",
      "        [2.1027e-01],\n",
      "        [7.4238e-03],\n",
      "        [9.9545e-01],\n",
      "        [8.5548e-03],\n",
      "        [1.0482e-01],\n",
      "        [1.4198e-01],\n",
      "        [3.6997e-01],\n",
      "        [5.2461e-01],\n",
      "        [8.4154e-04],\n",
      "        [2.4451e-01],\n",
      "        [4.9551e-01],\n",
      "        [4.4677e-01],\n",
      "        [1.0210e-01],\n",
      "        [1.3792e-03],\n",
      "        [3.0497e-01],\n",
      "        [8.9697e-02],\n",
      "        [8.1607e-01],\n",
      "        [6.2514e-01],\n",
      "        [5.9610e-02],\n",
      "        [3.3476e-01],\n",
      "        [6.9211e-02],\n",
      "        [2.1323e-01],\n",
      "        [8.1750e-04],\n",
      "        [1.4352e-01],\n",
      "        [9.2140e-04],\n",
      "        [6.4984e-01],\n",
      "        [8.7959e-01],\n",
      "        [1.4822e-01],\n",
      "        [1.5479e-01],\n",
      "        [1.0492e-01],\n",
      "        [1.2368e-03],\n",
      "        [5.3255e-01],\n",
      "        [6.5585e-01],\n",
      "        [3.2943e-01],\n",
      "        [4.2969e-01],\n",
      "        [9.9402e-01],\n",
      "        [3.8882e-01],\n",
      "        [2.3140e-01],\n",
      "        [4.1028e-01],\n",
      "        [4.2666e-01],\n",
      "        [9.9944e-04],\n",
      "        [4.7085e-01],\n",
      "        [9.4220e-04],\n",
      "        [3.5133e-01],\n",
      "        [4.9936e-01],\n",
      "        [5.7001e-01],\n",
      "        [8.5624e-01],\n",
      "        [7.6087e-01],\n",
      "        [2.9091e-01],\n",
      "        [1.5988e-01],\n",
      "        [2.8786e-02],\n",
      "        [3.0273e-01],\n",
      "        [9.8890e-04],\n",
      "        [5.2211e-01],\n",
      "        [3.3797e-01],\n",
      "        [4.1041e-01],\n",
      "        [2.0289e-01],\n",
      "        [1.0846e-03],\n",
      "        [3.0370e-01],\n",
      "        [2.2086e-03],\n",
      "        [1.7470e-03],\n",
      "        [9.2013e-04],\n",
      "        [2.1872e-03],\n",
      "        [6.4563e-01],\n",
      "        [1.1051e-03],\n",
      "        [7.7741e-01],\n",
      "        [3.9731e-01],\n",
      "        [3.1806e-01],\n",
      "        [9.1624e-01],\n",
      "        [6.4628e-01],\n",
      "        [5.9087e-01],\n",
      "        [1.8181e-03],\n",
      "        [5.9241e-01],\n",
      "        [8.0594e-04],\n",
      "        [4.0265e-01],\n",
      "        [4.9567e-02],\n",
      "        [9.9001e-02],\n",
      "        [1.2016e-03],\n",
      "        [2.5900e-01],\n",
      "        [6.8952e-01],\n",
      "        [4.6208e-01],\n",
      "        [3.8272e-01],\n",
      "        [1.2557e-03],\n",
      "        [8.3100e-02],\n",
      "        [4.4728e-01],\n",
      "        [1.5615e-03],\n",
      "        [1.8873e-01],\n",
      "        [5.9315e-01],\n",
      "        [4.0356e-01],\n",
      "        [2.7782e-01],\n",
      "        [6.8266e-01],\n",
      "        [1.7669e-01],\n",
      "        [6.2062e-01],\n",
      "        [1.6693e-01],\n",
      "        [5.3145e-01],\n",
      "        [7.3704e-01],\n",
      "        [2.0772e-01],\n",
      "        [1.2291e-01],\n",
      "        [7.3243e-04],\n",
      "        [4.3482e-01],\n",
      "        [7.6621e-02],\n",
      "        [5.2498e-01],\n",
      "        [2.4743e-03],\n",
      "        [4.0110e-01],\n",
      "        [2.3611e-03],\n",
      "        [3.0231e-01],\n",
      "        [7.8706e-04],\n",
      "        [2.5293e-01],\n",
      "        [1.2983e-03],\n",
      "        [3.3596e-01],\n",
      "        [1.1504e-03],\n",
      "        [6.2935e-01],\n",
      "        [2.9587e-01],\n",
      "        [1.5400e-03],\n",
      "        [1.0688e-03],\n",
      "        [1.1136e-03],\n",
      "        [1.1399e-03],\n",
      "        [1.2664e-03],\n",
      "        [5.3683e-01],\n",
      "        [3.9412e-01],\n",
      "        [5.8376e-01],\n",
      "        [1.4556e-01],\n",
      "        [3.1290e-01],\n",
      "        [1.0012e-03],\n",
      "        [1.2328e-01],\n",
      "        [7.1276e-01],\n",
      "        [1.5331e-03],\n",
      "        [8.4280e-04],\n",
      "        [1.8438e-01],\n",
      "        [5.1660e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "l1_error = 0\n",
    "l2_error = 0\n",
    "edge_probs = model(data)\n",
    "\n",
    "for i, e in enumerate(G.edges()):\n",
    "  u, v = e\n",
    "  p = G[u][v]['weight']\n",
    "  l1_error += abs(p - edge_probs[i].item())\n",
    "  l2_error += (p - edge_probs[i].item())**2\n",
    "\n",
    "print(l1_error)\n",
    "print(l1_error / G.number_of_edges())\n",
    "print(edge_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(G: nx.DiGraph, features):\n",
    "  # Create a PyG Data object from the networkx graph\n",
    "  edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "  #x = torch.tensor(features, dtype=torch.float)\n",
    "  data = Data(x=features, edge_index=edge_index)\n",
    "  return data\n",
    "\n",
    "n = 100\n",
    "m0 = 2\n",
    "p = 0.1\n",
    "a = 0.41\n",
    "b = 0.54\n",
    "c = 0.05\n",
    "gname = f\"er_{n}_{str(p).replace('.', '')}\"\n",
    "#gname = \"congress-twitter\"\n",
    "#gname = f\"sf_{n}_{str(a).replace('.', '')}_{str(b).replace('.', '')}_{str(c).replace('.', '')}\"\n",
    "#gname = f\"ba_{n}_{m0}\"\n",
    "path = Path(f\"datasets/synthetic/{gname}\")\n",
    "\n",
    "with open(path / f\"{gname}.mtx\", \"rb\") as fh:\n",
    "  G = nx.from_scipy_sparse_array(sp.io.mmread(fh), create_using=nx.DiGraph)\n",
    "\n",
    "cascades = []\n",
    "idxes = rng.choice(1000, 100, replace=False)\n",
    "for i in idxes:\n",
    "  with open(path / f\"diffusions/timestamps/{i}.txt\", \"r\") as fh:\n",
    "    cascade = []\n",
    "    for line in fh:\n",
    "      cascade.append(list(map(int, line.strip().split())))\n",
    "    cascades.append(cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Avg Loss: 121.3335\n",
      "Epoch 11/50, Avg Loss: 103.9308\n",
      "Epoch 21/50, Avg Loss: 102.3731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m GNNIndependentCascade(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m16\u001b[39m, n, m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     75\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcascades\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcascades\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, data, cascades, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     59\u001b[0m edge_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(data)\n\u001b[0;32m---> 61\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(edge_probs, edge_index, cascades)\u001b[0m\n\u001b[1;32m     90\u001b[0m total_log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cascade \u001b[38;5;129;01min\u001b[39;00m cascades:\n\u001b[0;32m---> 92\u001b[0m   total_log_likelihood \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cascade_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcascade\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Return negative log-likelihood as the loss\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mtotal_log_likelihood\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mcompute_cascade_likelihood\u001b[0;34m(edge_probs, edge_index, cascade, epsilon)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m curr_activated:\n\u001b[1;32m     29\u001b[0m   parents \u001b[38;5;241m=\u001b[39m src[(dst \u001b[38;5;241m==\u001b[39m v) \u001b[38;5;241m&\u001b[39m activated[src]]\n\u001b[0;32m---> 30\u001b[0m   activated_parents \u001b[38;5;241m=\u001b[39m parents[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_activated\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(activated_parents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m     prob_v_activated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mprod(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m edge_probs[torch\u001b[38;5;241m.\u001b[39misin(src, activated_parents) \u001b[38;5;241m&\u001b[39m (dst \u001b[38;5;241m==\u001b[39m v)])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GNNIndependentCascade(torch.nn.Module):\n",
    "  def __init__(self, node_emb_dim, hidden_dim, n_nodes, n_edges, num_layers=2):\n",
    "    super(GNNIndependentCascade, self).__init__()\n",
    "    self.n = n_nodes\n",
    "    self.m = n_edges\n",
    "\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.node_embed = nn.Embedding(self.n, node_emb_dim)\n",
    "    self.edge_embed = nn.Parameter(torch.Tensor(self.n, self.n, hidden_dim))\n",
    "\n",
    "    self.convs = nn.ModuleList([\n",
    "      GATConv(node_emb_dim if i == 0 else hidden_dim, hidden_dim) \n",
    "      for i in range(num_layers)\n",
    "    ])\n",
    "\n",
    "    self.edge_predictor = nn.Sequential(\n",
    "      nn.Linear(2*hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, 1),\n",
    "    )\n",
    "    #nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, data):\n",
    "    edge_index = data.edge_index\n",
    "    x = self.node_embed(torch.arange(self.n))\n",
    "\n",
    "    src, dst = edge_index\n",
    "    edge_emb = self.edge_embed[src, dst]\n",
    "\n",
    "    # Node embedding\n",
    "    for i in range(self.num_layers):\n",
    "      #x = self.convs[i](x, edge_index, edge_emb)\n",
    "      x = self.convs[i](x, edge_index)\n",
    "      x = F.gelu(x)\n",
    "      x = torch.dropout(x, p=0.1, train=self.training)\n",
    "\n",
    "    # Edge probability prediction\n",
    "    #edge_repr = torch.cat([x[src], x[dst]], dim=1)\n",
    "    edge_repr = torch.cat([torch.add(x[src], x[dst]), edge_emb], dim=1)\n",
    "    edge_probs = torch.sigmoid(self.edge_predictor(edge_repr))\n",
    "\n",
    "    #edge_probs = torch.sigmoid(torch.sum(x[row] * x[col], dim=1))\n",
    "\n",
    "    return edge_probs\n",
    "\n",
    "def train_model(model, optimizer, data, cascades, num_epochs, batch_size = 50):\n",
    "  model.train()\n",
    "  #batches = DataLoader(cascades, batch_size=10, shuffle=True)\n",
    "  #print(batches)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    loss = 0.0\n",
    "    rng.shuffle(cascades)\n",
    "    batches = [cascades[i:i+batch_size] for i in range(0, len(cascades), batch_size)]\n",
    "\n",
    "    for batch in batches:\n",
    "      optimizer.zero_grad()\n",
    "      edge_probs = model.forward(data)\n",
    "\n",
    "      batch_loss = compute_loss(edge_probs, data.edge_index, batch)\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      loss += batch_loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {loss / len(cascades):.4f}\")\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "features = torch.eye(n)\n",
    "data = create_dataset(G, features)\n",
    "\n",
    "model = GNNIndependentCascade(64, 16, n, m, num_layers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "train_model(model, optimizer, data, cascades, 50, len(cascades) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Error: 0.10014827800129067\n",
      "Median: 0.08214325020790098, Q1: 0.03609595981955528, Q3: 0.1434874703884125\n",
      "tensor([[0.2388],\n",
      "        [0.1588],\n",
      "        [0.5576],\n",
      "        [0.1086],\n",
      "        [0.4183],\n",
      "        [0.6892],\n",
      "        [0.1740],\n",
      "        [0.3602],\n",
      "        [0.0147],\n",
      "        [0.1760],\n",
      "        [0.7309],\n",
      "        [0.2587],\n",
      "        [0.3410],\n",
      "        [0.1270],\n",
      "        [0.4631],\n",
      "        [0.3812],\n",
      "        [0.3395],\n",
      "        [0.5676],\n",
      "        [0.2961],\n",
      "        [0.5747],\n",
      "        [0.1874],\n",
      "        [0.4196],\n",
      "        [0.4219],\n",
      "        [0.3334],\n",
      "        [0.2243],\n",
      "        [0.1855],\n",
      "        [0.3464],\n",
      "        [0.4313],\n",
      "        [0.3453],\n",
      "        [0.5353],\n",
      "        [0.2628],\n",
      "        [0.7263],\n",
      "        [0.1158],\n",
      "        [0.2674],\n",
      "        [0.0904],\n",
      "        [0.4271],\n",
      "        [0.0665],\n",
      "        [0.2159],\n",
      "        [0.4200],\n",
      "        [0.5233],\n",
      "        [0.6858],\n",
      "        [0.0356],\n",
      "        [0.0101],\n",
      "        [0.1339],\n",
      "        [0.7061],\n",
      "        [0.3281],\n",
      "        [0.3773],\n",
      "        [0.4297],\n",
      "        [0.0204],\n",
      "        [0.4501],\n",
      "        [0.3827],\n",
      "        [0.2089],\n",
      "        [0.2485],\n",
      "        [0.4881],\n",
      "        [0.2744],\n",
      "        [0.3627],\n",
      "        [0.2448],\n",
      "        [0.1279],\n",
      "        [0.2931],\n",
      "        [0.5545],\n",
      "        [0.1739],\n",
      "        [0.7366],\n",
      "        [0.2308],\n",
      "        [0.3278],\n",
      "        [0.2432],\n",
      "        [0.3265],\n",
      "        [0.4180],\n",
      "        [0.1789],\n",
      "        [0.5335],\n",
      "        [0.5903],\n",
      "        [0.2037],\n",
      "        [0.3502],\n",
      "        [0.3771],\n",
      "        [0.4105],\n",
      "        [0.2338],\n",
      "        [0.1195],\n",
      "        [0.2605],\n",
      "        [0.2807],\n",
      "        [0.2727],\n",
      "        [0.5014],\n",
      "        [0.2578],\n",
      "        [0.3320],\n",
      "        [0.2334],\n",
      "        [0.4289],\n",
      "        [0.1779],\n",
      "        [0.0524],\n",
      "        [0.1711],\n",
      "        [0.2924],\n",
      "        [0.2226],\n",
      "        [0.0704],\n",
      "        [0.5114],\n",
      "        [0.5062],\n",
      "        [0.5631],\n",
      "        [0.2777],\n",
      "        [0.0269],\n",
      "        [0.0122],\n",
      "        [0.7152],\n",
      "        [0.3940],\n",
      "        [0.1806],\n",
      "        [0.3608],\n",
      "        [0.3508],\n",
      "        [0.2477],\n",
      "        [0.2584],\n",
      "        [0.3052],\n",
      "        [0.3887],\n",
      "        [0.2368],\n",
      "        [0.4184],\n",
      "        [0.4972],\n",
      "        [0.3593],\n",
      "        [0.0846],\n",
      "        [0.3353],\n",
      "        [0.4008],\n",
      "        [0.2457],\n",
      "        [0.0459],\n",
      "        [0.6377],\n",
      "        [0.6152],\n",
      "        [0.1917],\n",
      "        [0.0629],\n",
      "        [0.3592],\n",
      "        [0.3999],\n",
      "        [0.2841],\n",
      "        [0.4055],\n",
      "        [0.1275],\n",
      "        [0.2568],\n",
      "        [0.1880],\n",
      "        [0.2920],\n",
      "        [0.4349],\n",
      "        [0.5008],\n",
      "        [0.0655],\n",
      "        [0.3247],\n",
      "        [0.4711],\n",
      "        [0.1463],\n",
      "        [0.2039],\n",
      "        [0.5692],\n",
      "        [0.5836],\n",
      "        [0.3138],\n",
      "        [0.2272],\n",
      "        [0.0113],\n",
      "        [0.2243],\n",
      "        [0.4397],\n",
      "        [0.0852],\n",
      "        [0.2440],\n",
      "        [0.1627],\n",
      "        [0.2460],\n",
      "        [0.3745],\n",
      "        [0.3532],\n",
      "        [0.0119],\n",
      "        [0.4556],\n",
      "        [0.1924],\n",
      "        [0.0214],\n",
      "        [0.3268],\n",
      "        [0.2432],\n",
      "        [0.2353],\n",
      "        [0.7234],\n",
      "        [0.0117],\n",
      "        [0.1625],\n",
      "        [0.2198],\n",
      "        [0.2825],\n",
      "        [0.0154],\n",
      "        [0.4073],\n",
      "        [0.3940],\n",
      "        [0.2378],\n",
      "        [0.1912],\n",
      "        [0.4179],\n",
      "        [0.2687],\n",
      "        [0.1740],\n",
      "        [0.4385],\n",
      "        [0.1975],\n",
      "        [0.0255],\n",
      "        [0.3601],\n",
      "        [0.6810],\n",
      "        [0.2578],\n",
      "        [0.4933],\n",
      "        [0.2402],\n",
      "        [0.5131],\n",
      "        [0.4352],\n",
      "        [0.0416],\n",
      "        [0.1907],\n",
      "        [0.3193],\n",
      "        [0.1977],\n",
      "        [0.3112],\n",
      "        [0.1591],\n",
      "        [0.1949],\n",
      "        [0.5313],\n",
      "        [0.6508],\n",
      "        [0.2099],\n",
      "        [0.0149],\n",
      "        [0.4283],\n",
      "        [0.0150],\n",
      "        [0.2030],\n",
      "        [0.3736],\n",
      "        [0.4708],\n",
      "        [0.3259],\n",
      "        [0.3031],\n",
      "        [0.3869],\n",
      "        [0.3586],\n",
      "        [0.3099],\n",
      "        [0.5665],\n",
      "        [0.2930],\n",
      "        [0.1820],\n",
      "        [0.1768],\n",
      "        [0.1303],\n",
      "        [0.5798],\n",
      "        [0.3230],\n",
      "        [0.4858],\n",
      "        [0.1427],\n",
      "        [0.6336],\n",
      "        [0.2273],\n",
      "        [0.2669],\n",
      "        [0.5725],\n",
      "        [0.0095],\n",
      "        [0.3378],\n",
      "        [0.5033],\n",
      "        [0.4045],\n",
      "        [0.4641],\n",
      "        [0.1045],\n",
      "        [0.3309],\n",
      "        [0.0540],\n",
      "        [0.0176],\n",
      "        [0.2515],\n",
      "        [0.1219],\n",
      "        [0.2123],\n",
      "        [0.1139],\n",
      "        [0.7302],\n",
      "        [0.5588],\n",
      "        [0.2898],\n",
      "        [0.4593],\n",
      "        [0.3913],\n",
      "        [0.5195],\n",
      "        [0.0708],\n",
      "        [0.6781],\n",
      "        [0.3524],\n",
      "        [0.0111],\n",
      "        [0.1227],\n",
      "        [0.1195],\n",
      "        [0.2238],\n",
      "        [0.3107],\n",
      "        [0.0647],\n",
      "        [0.1194],\n",
      "        [0.4409],\n",
      "        [0.0231],\n",
      "        [0.0809],\n",
      "        [0.0196],\n",
      "        [0.0109],\n",
      "        [0.4898],\n",
      "        [0.5205],\n",
      "        [0.0903],\n",
      "        [0.0162],\n",
      "        [0.5239],\n",
      "        [0.2957],\n",
      "        [0.3203],\n",
      "        [0.1668],\n",
      "        [0.4397],\n",
      "        [0.6942],\n",
      "        [0.1810],\n",
      "        [0.2425],\n",
      "        [0.1894],\n",
      "        [0.2436],\n",
      "        [0.3887],\n",
      "        [0.2517],\n",
      "        [0.0193],\n",
      "        [0.1661],\n",
      "        [0.1095],\n",
      "        [0.4862],\n",
      "        [0.2287],\n",
      "        [0.4070],\n",
      "        [0.5855],\n",
      "        [0.3858],\n",
      "        [0.0216],\n",
      "        [0.5890],\n",
      "        [0.4750],\n",
      "        [0.0200],\n",
      "        [0.0596],\n",
      "        [0.7957],\n",
      "        [0.0928],\n",
      "        [0.4270],\n",
      "        [0.4016],\n",
      "        [0.4959],\n",
      "        [0.5439],\n",
      "        [0.4745],\n",
      "        [0.0127],\n",
      "        [0.4328],\n",
      "        [0.6124],\n",
      "        [0.2944],\n",
      "        [0.3163],\n",
      "        [0.2891],\n",
      "        [0.1703],\n",
      "        [0.1980],\n",
      "        [0.2855],\n",
      "        [0.3080],\n",
      "        [0.0484],\n",
      "        [0.3273],\n",
      "        [0.2240],\n",
      "        [0.2630],\n",
      "        [0.3049],\n",
      "        [0.0488],\n",
      "        [0.0130],\n",
      "        [0.3147],\n",
      "        [0.3191],\n",
      "        [0.4710],\n",
      "        [0.4303],\n",
      "        [0.1569],\n",
      "        [0.2583],\n",
      "        [0.3138],\n",
      "        [0.3126],\n",
      "        [0.1857],\n",
      "        [0.3380],\n",
      "        [0.0146],\n",
      "        [0.4444],\n",
      "        [0.2438],\n",
      "        [0.4961],\n",
      "        [0.0131],\n",
      "        [0.4552],\n",
      "        [0.1073],\n",
      "        [0.3277],\n",
      "        [0.5319],\n",
      "        [0.0489],\n",
      "        [0.6202],\n",
      "        [0.3001],\n",
      "        [0.4864],\n",
      "        [0.1245],\n",
      "        [0.0999],\n",
      "        [0.0312],\n",
      "        [0.2185],\n",
      "        [0.0673],\n",
      "        [0.2522],\n",
      "        [0.4479],\n",
      "        [0.2820],\n",
      "        [0.4104],\n",
      "        [0.2483],\n",
      "        [0.2632],\n",
      "        [0.0623],\n",
      "        [0.5169],\n",
      "        [0.3952],\n",
      "        [0.4438],\n",
      "        [0.5044],\n",
      "        [0.5393],\n",
      "        [0.3511],\n",
      "        [0.0195],\n",
      "        [0.2020],\n",
      "        [0.3285],\n",
      "        [0.2679],\n",
      "        [0.4518],\n",
      "        [0.6988],\n",
      "        [0.4909],\n",
      "        [0.5359],\n",
      "        [0.0053],\n",
      "        [0.0291],\n",
      "        [0.1543],\n",
      "        [0.3683],\n",
      "        [0.3090],\n",
      "        [0.0326],\n",
      "        [0.3362],\n",
      "        [0.3004],\n",
      "        [0.5371],\n",
      "        [0.6563],\n",
      "        [0.4963],\n",
      "        [0.4574],\n",
      "        [0.3666],\n",
      "        [0.6385],\n",
      "        [0.1481],\n",
      "        [0.0554],\n",
      "        [0.2911],\n",
      "        [0.4816],\n",
      "        [0.0098],\n",
      "        [0.1726],\n",
      "        [0.4087],\n",
      "        [0.2806],\n",
      "        [0.4828],\n",
      "        [0.2725],\n",
      "        [0.2394],\n",
      "        [0.4309],\n",
      "        [0.1899],\n",
      "        [0.3159],\n",
      "        [0.5895],\n",
      "        [0.1642],\n",
      "        [0.3114],\n",
      "        [0.1837],\n",
      "        [0.0085],\n",
      "        [0.5158],\n",
      "        [0.3175],\n",
      "        [0.2752],\n",
      "        [0.0878],\n",
      "        [0.0135],\n",
      "        [0.5184],\n",
      "        [0.1520],\n",
      "        [0.4223],\n",
      "        [0.3602],\n",
      "        [0.5231],\n",
      "        [0.4356],\n",
      "        [0.6161],\n",
      "        [0.1079],\n",
      "        [0.4387],\n",
      "        [0.7589],\n",
      "        [0.2468],\n",
      "        [0.2974],\n",
      "        [0.2372],\n",
      "        [0.1396],\n",
      "        [0.1647],\n",
      "        [0.4809],\n",
      "        [0.3840],\n",
      "        [0.2288],\n",
      "        [0.6809],\n",
      "        [0.5560],\n",
      "        [0.1832],\n",
      "        [0.3755],\n",
      "        [0.4029],\n",
      "        [0.6970],\n",
      "        [0.6470],\n",
      "        [0.5935],\n",
      "        [0.0457],\n",
      "        [0.2869],\n",
      "        [0.2742],\n",
      "        [0.7226],\n",
      "        [0.0256],\n",
      "        [0.0713],\n",
      "        [0.0690],\n",
      "        [0.2172],\n",
      "        [0.2282],\n",
      "        [0.0161],\n",
      "        [0.0223],\n",
      "        [0.0959],\n",
      "        [0.6952],\n",
      "        [0.2565],\n",
      "        [0.1039],\n",
      "        [0.0350],\n",
      "        [0.4261],\n",
      "        [0.4928],\n",
      "        [0.3495],\n",
      "        [0.0116],\n",
      "        [0.7367],\n",
      "        [0.3516],\n",
      "        [0.1761],\n",
      "        [0.0133],\n",
      "        [0.5358],\n",
      "        [0.0147],\n",
      "        [0.3163],\n",
      "        [0.5074],\n",
      "        [0.6441],\n",
      "        [0.6804],\n",
      "        [0.0655],\n",
      "        [0.0193],\n",
      "        [0.5864],\n",
      "        [0.6915],\n",
      "        [0.5627],\n",
      "        [0.0579],\n",
      "        [0.4222],\n",
      "        [0.2546],\n",
      "        [0.3886],\n",
      "        [0.0103],\n",
      "        [0.4913],\n",
      "        [0.4455],\n",
      "        [0.6317],\n",
      "        [0.2174],\n",
      "        [0.1355],\n",
      "        [0.4810],\n",
      "        [0.2276],\n",
      "        [0.6781],\n",
      "        [0.3514],\n",
      "        [0.3624],\n",
      "        [0.3142],\n",
      "        [0.4606],\n",
      "        [0.3757],\n",
      "        [0.4687],\n",
      "        [0.5598],\n",
      "        [0.0114],\n",
      "        [0.6729],\n",
      "        [0.2494],\n",
      "        [0.0191],\n",
      "        [0.3006],\n",
      "        [0.4884],\n",
      "        [0.1336],\n",
      "        [0.2318],\n",
      "        [0.1807],\n",
      "        [0.1823],\n",
      "        [0.0853],\n",
      "        [0.0531],\n",
      "        [0.3779],\n",
      "        [0.3713],\n",
      "        [0.3918],\n",
      "        [0.3964],\n",
      "        [0.0204],\n",
      "        [0.5479],\n",
      "        [0.5351],\n",
      "        [0.3617],\n",
      "        [0.3528],\n",
      "        [0.1608],\n",
      "        [0.2294],\n",
      "        [0.1765],\n",
      "        [0.7192],\n",
      "        [0.4877],\n",
      "        [0.5173],\n",
      "        [0.0180],\n",
      "        [0.5268],\n",
      "        [0.0806],\n",
      "        [0.4996],\n",
      "        [0.3732],\n",
      "        [0.3474],\n",
      "        [0.3415],\n",
      "        [0.0784],\n",
      "        [0.2287],\n",
      "        [0.2616],\n",
      "        [0.4972],\n",
      "        [0.0608],\n",
      "        [0.3841],\n",
      "        [0.2047],\n",
      "        [0.0114],\n",
      "        [0.0674],\n",
      "        [0.3559],\n",
      "        [0.3577],\n",
      "        [0.2147],\n",
      "        [0.2039],\n",
      "        [0.2693],\n",
      "        [0.2055],\n",
      "        [0.5099],\n",
      "        [0.1587],\n",
      "        [0.0212],\n",
      "        [0.4029],\n",
      "        [0.6559],\n",
      "        [0.3655],\n",
      "        [0.5976],\n",
      "        [0.2485],\n",
      "        [0.5472],\n",
      "        [0.4909],\n",
      "        [0.1085],\n",
      "        [0.5835],\n",
      "        [0.3350],\n",
      "        [0.4118],\n",
      "        [0.1581],\n",
      "        [0.6514],\n",
      "        [0.1057],\n",
      "        [0.0417],\n",
      "        [0.6003],\n",
      "        [0.5086],\n",
      "        [0.0144],\n",
      "        [0.2618],\n",
      "        [0.3506],\n",
      "        [0.0122],\n",
      "        [0.3270],\n",
      "        [0.3602],\n",
      "        [0.4340],\n",
      "        [0.5706],\n",
      "        [0.3896],\n",
      "        [0.4515],\n",
      "        [0.3731],\n",
      "        [0.3214],\n",
      "        [0.1996],\n",
      "        [0.4115],\n",
      "        [0.4263],\n",
      "        [0.2713],\n",
      "        [0.0364],\n",
      "        [0.1177],\n",
      "        [0.6805],\n",
      "        [0.2247],\n",
      "        [0.2101],\n",
      "        [0.6353],\n",
      "        [0.4830],\n",
      "        [0.0169],\n",
      "        [0.4028],\n",
      "        [0.4856],\n",
      "        [0.2711],\n",
      "        [0.4382],\n",
      "        [0.1067],\n",
      "        [0.4295],\n",
      "        [0.5608],\n",
      "        [0.0790],\n",
      "        [0.2161],\n",
      "        [0.1539],\n",
      "        [0.0117],\n",
      "        [0.4784],\n",
      "        [0.4593],\n",
      "        [0.0112],\n",
      "        [0.4378],\n",
      "        [0.2104],\n",
      "        [0.5499],\n",
      "        [0.4776],\n",
      "        [0.7147],\n",
      "        [0.5351],\n",
      "        [0.3363],\n",
      "        [0.0818],\n",
      "        [0.4716],\n",
      "        [0.3051],\n",
      "        [0.4601],\n",
      "        [0.0550],\n",
      "        [0.2359],\n",
      "        [0.4745],\n",
      "        [0.1560],\n",
      "        [0.0134],\n",
      "        [0.0113],\n",
      "        [0.1449],\n",
      "        [0.4399],\n",
      "        [0.2058],\n",
      "        [0.3363],\n",
      "        [0.2763],\n",
      "        [0.5765],\n",
      "        [0.5131],\n",
      "        [0.5042],\n",
      "        [0.0178],\n",
      "        [0.1366],\n",
      "        [0.4889],\n",
      "        [0.5878],\n",
      "        [0.7447],\n",
      "        [0.3482],\n",
      "        [0.4533],\n",
      "        [0.7353],\n",
      "        [0.4469],\n",
      "        [0.1731],\n",
      "        [0.2823],\n",
      "        [0.1450],\n",
      "        [0.0972],\n",
      "        [0.1652],\n",
      "        [0.2615],\n",
      "        [0.2790],\n",
      "        [0.2570],\n",
      "        [0.0951],\n",
      "        [0.0367],\n",
      "        [0.3351],\n",
      "        [0.2428],\n",
      "        [0.1168],\n",
      "        [0.1474],\n",
      "        [0.2020],\n",
      "        [0.0130],\n",
      "        [0.3523],\n",
      "        [0.4780],\n",
      "        [0.4522],\n",
      "        [0.3944],\n",
      "        [0.6845],\n",
      "        [0.4927],\n",
      "        [0.1533],\n",
      "        [0.5126],\n",
      "        [0.1639],\n",
      "        [0.1588],\n",
      "        [0.4598],\n",
      "        [0.3259],\n",
      "        [0.1878],\n",
      "        [0.4153],\n",
      "        [0.3825],\n",
      "        [0.3433],\n",
      "        [0.2611],\n",
      "        [0.4815],\n",
      "        [0.4643],\n",
      "        [0.5701],\n",
      "        [0.0509],\n",
      "        [0.5217],\n",
      "        [0.0999],\n",
      "        [0.3080],\n",
      "        [0.4877],\n",
      "        [0.2904],\n",
      "        [0.3368],\n",
      "        [0.1115],\n",
      "        [0.2906],\n",
      "        [0.2360],\n",
      "        [0.5516],\n",
      "        [0.5190],\n",
      "        [0.1301],\n",
      "        [0.0731],\n",
      "        [0.0891],\n",
      "        [0.4293],\n",
      "        [0.2011],\n",
      "        [0.4859],\n",
      "        [0.0190],\n",
      "        [0.1419],\n",
      "        [0.0154],\n",
      "        [0.0297],\n",
      "        [0.1235],\n",
      "        [0.6056],\n",
      "        [0.3664],\n",
      "        [0.5169],\n",
      "        [0.4266],\n",
      "        [0.2233],\n",
      "        [0.3712],\n",
      "        [0.0249],\n",
      "        [0.6302],\n",
      "        [0.2874],\n",
      "        [0.2608],\n",
      "        [0.5968],\n",
      "        [0.1917],\n",
      "        [0.2115],\n",
      "        [0.3490],\n",
      "        [0.5379],\n",
      "        [0.2835],\n",
      "        [0.3207],\n",
      "        [0.2807],\n",
      "        [0.2682],\n",
      "        [0.2034],\n",
      "        [0.4078],\n",
      "        [0.5749],\n",
      "        [0.4114],\n",
      "        [0.1456],\n",
      "        [0.2316],\n",
      "        [0.4466],\n",
      "        [0.3923],\n",
      "        [0.3783],\n",
      "        [0.2517],\n",
      "        [0.4480],\n",
      "        [0.4038],\n",
      "        [0.4206],\n",
      "        [0.0132],\n",
      "        [0.2741],\n",
      "        [0.3674],\n",
      "        [0.3581],\n",
      "        [0.0209],\n",
      "        [0.3567],\n",
      "        [0.0154],\n",
      "        [0.3240],\n",
      "        [0.2389],\n",
      "        [0.2831],\n",
      "        [0.2377],\n",
      "        [0.0110],\n",
      "        [0.4055],\n",
      "        [0.1269],\n",
      "        [0.2026],\n",
      "        [0.3318],\n",
      "        [0.5548],\n",
      "        [0.3207],\n",
      "        [0.0103],\n",
      "        [0.2458],\n",
      "        [0.1773],\n",
      "        [0.0141],\n",
      "        [0.4639],\n",
      "        [0.3212],\n",
      "        [0.1386],\n",
      "        [0.4796],\n",
      "        [0.3712],\n",
      "        [0.0154],\n",
      "        [0.6310],\n",
      "        [0.0109],\n",
      "        [0.2784],\n",
      "        [0.4779],\n",
      "        [0.0952],\n",
      "        [0.5230],\n",
      "        [0.5501],\n",
      "        [0.3422],\n",
      "        [0.4946],\n",
      "        [0.0155],\n",
      "        [0.1016],\n",
      "        [0.4693],\n",
      "        [0.3222],\n",
      "        [0.5418],\n",
      "        [0.3170],\n",
      "        [0.4877],\n",
      "        [0.5537],\n",
      "        [0.4812],\n",
      "        [0.2760],\n",
      "        [0.4850],\n",
      "        [0.2882],\n",
      "        [0.3413],\n",
      "        [0.6210],\n",
      "        [0.2774],\n",
      "        [0.5450],\n",
      "        [0.1645],\n",
      "        [0.1953],\n",
      "        [0.1966],\n",
      "        [0.1865],\n",
      "        [0.6671],\n",
      "        [0.4428],\n",
      "        [0.2974],\n",
      "        [0.2278],\n",
      "        [0.4279],\n",
      "        [0.7067],\n",
      "        [0.3479],\n",
      "        [0.2998],\n",
      "        [0.2218],\n",
      "        [0.2213],\n",
      "        [0.4677],\n",
      "        [0.3628],\n",
      "        [0.2922],\n",
      "        [0.5622],\n",
      "        [0.4787],\n",
      "        [0.0142],\n",
      "        [0.1116],\n",
      "        [0.1216],\n",
      "        [0.3463],\n",
      "        [0.1292],\n",
      "        [0.2764],\n",
      "        [0.5440],\n",
      "        [0.0120],\n",
      "        [0.4605],\n",
      "        [0.4169],\n",
      "        [0.1535],\n",
      "        [0.4596],\n",
      "        [0.4828],\n",
      "        [0.4918],\n",
      "        [0.0196],\n",
      "        [0.0208],\n",
      "        [0.1819],\n",
      "        [0.0803],\n",
      "        [0.5240],\n",
      "        [0.1517],\n",
      "        [0.1795],\n",
      "        [0.2623],\n",
      "        [0.3305],\n",
      "        [0.2618],\n",
      "        [0.2216],\n",
      "        [0.4851],\n",
      "        [0.1471],\n",
      "        [0.0127],\n",
      "        [0.2668],\n",
      "        [0.4093],\n",
      "        [0.5952],\n",
      "        [0.6432],\n",
      "        [0.4003],\n",
      "        [0.3005],\n",
      "        [0.0236],\n",
      "        [0.3534],\n",
      "        [0.4428],\n",
      "        [0.1227],\n",
      "        [0.4094],\n",
      "        [0.6065],\n",
      "        [0.2195],\n",
      "        [0.2123],\n",
      "        [0.2066],\n",
      "        [0.2042],\n",
      "        [0.2304],\n",
      "        [0.2117],\n",
      "        [0.5277],\n",
      "        [0.1409],\n",
      "        [0.3672],\n",
      "        [0.2731],\n",
      "        [0.0776],\n",
      "        [0.3347],\n",
      "        [0.3791],\n",
      "        [0.4173],\n",
      "        [0.0598],\n",
      "        [0.0809],\n",
      "        [0.3930],\n",
      "        [0.5833],\n",
      "        [0.4100],\n",
      "        [0.7250],\n",
      "        [0.0117],\n",
      "        [0.3725],\n",
      "        [0.3249],\n",
      "        [0.4583],\n",
      "        [0.4306],\n",
      "        [0.1488],\n",
      "        [0.4613],\n",
      "        [0.2294],\n",
      "        [0.1047],\n",
      "        [0.4421],\n",
      "        [0.0142],\n",
      "        [0.4852],\n",
      "        [0.3844],\n",
      "        [0.2771],\n",
      "        [0.1985],\n",
      "        [0.6921],\n",
      "        [0.4691],\n",
      "        [0.6432],\n",
      "        [0.1691],\n",
      "        [0.4146],\n",
      "        [0.4219],\n",
      "        [0.1065],\n",
      "        [0.7263],\n",
      "        [0.1470],\n",
      "        [0.3185],\n",
      "        [0.7366],\n",
      "        [0.4193],\n",
      "        [0.4364],\n",
      "        [0.5470],\n",
      "        [0.2855],\n",
      "        [0.5217],\n",
      "        [0.3930],\n",
      "        [0.3672],\n",
      "        [0.5297],\n",
      "        [0.2494],\n",
      "        [0.4108],\n",
      "        [0.3027],\n",
      "        [0.0298],\n",
      "        [0.3525],\n",
      "        [0.6054],\n",
      "        [0.2277],\n",
      "        [0.2044],\n",
      "        [0.6693],\n",
      "        [0.5865],\n",
      "        [0.2826],\n",
      "        [0.3735],\n",
      "        [0.1908],\n",
      "        [0.2240],\n",
      "        [0.6348],\n",
      "        [0.4914],\n",
      "        [0.2899],\n",
      "        [0.1949],\n",
      "        [0.1850],\n",
      "        [0.4732],\n",
      "        [0.6692],\n",
      "        [0.6981],\n",
      "        [0.3290],\n",
      "        [0.5135],\n",
      "        [0.5132],\n",
      "        [0.0134],\n",
      "        [0.0673],\n",
      "        [0.3622],\n",
      "        [0.5039],\n",
      "        [0.2297],\n",
      "        [0.4514],\n",
      "        [0.2379],\n",
      "        [0.3671],\n",
      "        [0.2521],\n",
      "        [0.6740],\n",
      "        [0.0182],\n",
      "        [0.3430],\n",
      "        [0.0181],\n",
      "        [0.4296],\n",
      "        [0.2184],\n",
      "        [0.4274],\n",
      "        [0.0619],\n",
      "        [0.1492],\n",
      "        [0.1293],\n",
      "        [0.2481],\n",
      "        [0.0156],\n",
      "        [0.0216],\n",
      "        [0.3427],\n",
      "        [0.5573],\n",
      "        [0.4708],\n",
      "        [0.1475],\n",
      "        [0.4038],\n",
      "        [0.0459],\n",
      "        [0.3924],\n",
      "        [0.2348],\n",
      "        [0.3585],\n",
      "        [0.2537],\n",
      "        [0.2456],\n",
      "        [0.3235],\n",
      "        [0.3348],\n",
      "        [0.5132],\n",
      "        [0.2542],\n",
      "        [0.2785],\n",
      "        [0.2349],\n",
      "        [0.4646],\n",
      "        [0.2537],\n",
      "        [0.0598],\n",
      "        [0.1702],\n",
      "        [0.4831],\n",
      "        [0.3803],\n",
      "        [0.2503],\n",
      "        [0.0706],\n",
      "        [0.6775],\n",
      "        [0.2737],\n",
      "        [0.5453],\n",
      "        [0.1961],\n",
      "        [0.2298],\n",
      "        [0.5158],\n",
      "        [0.2701],\n",
      "        [0.4398],\n",
      "        [0.3791],\n",
      "        [0.4511],\n",
      "        [0.3016],\n",
      "        [0.2287],\n",
      "        [0.2452],\n",
      "        [0.3616],\n",
      "        [0.1612],\n",
      "        [0.3420],\n",
      "        [0.1676],\n",
      "        [0.1589],\n",
      "        [0.1825],\n",
      "        [0.0292],\n",
      "        [0.3535],\n",
      "        [0.5211],\n",
      "        [0.4190],\n",
      "        [0.5393],\n",
      "        [0.0973],\n",
      "        [0.1946],\n",
      "        [0.4394],\n",
      "        [0.4758],\n",
      "        [0.0130],\n",
      "        [0.2815],\n",
      "        [0.5735],\n",
      "        [0.5440],\n",
      "        [0.2723],\n",
      "        [0.2870],\n",
      "        [0.2549],\n",
      "        [0.0137],\n",
      "        [0.1551],\n",
      "        [0.4599],\n",
      "        [0.1883],\n",
      "        [0.5679],\n",
      "        [0.4661],\n",
      "        [0.7253],\n",
      "        [0.2379],\n",
      "        [0.2279],\n",
      "        [0.0441],\n",
      "        [0.3033],\n",
      "        [0.2147],\n",
      "        [0.0530],\n",
      "        [0.0379],\n",
      "        [0.5705],\n",
      "        [0.3064],\n",
      "        [0.5142]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "edge_probs = model(data)\n",
    "\n",
    "residuals = []\n",
    "for i, e in enumerate(G.edges()):\n",
    "  u, v = e\n",
    "  p = G[u][v]['weight']\n",
    "  residuals.append(abs(p - edge_probs[i].item()))\n",
    "\n",
    "l1_error = sum(residuals) / len(residuals)\n",
    "median = np.median(residuals)\n",
    "quartile1 = np.percentile(residuals, 25) \n",
    "quartile3 = np.percentile(residuals, 75)\n",
    "\n",
    "print(f\"MAE Error: {l1_error}\")\n",
    "print(f\"Median: {median}, Q1: {quartile1}, Q3: {quartile3}\")\n",
    "print(edge_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
