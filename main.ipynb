{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, Linear, SAGEConv, GATv2Conv, GATConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy as sp\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNIndependentCascade(torch.nn.Module):\n",
    "  def __init__(self, num_node_features, hidden_dim, num_layers=2):\n",
    "    super(GNNIndependentCascade, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.convs = nn.ModuleList([\n",
    "      GATConv(num_node_features if i == 0 else hidden_dim, hidden_dim) \n",
    "      for i in range(num_layers)]\n",
    "    )\n",
    "\n",
    "    self.edge_predictor = nn.Sequential(\n",
    "      nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "      nn.ELU(),\n",
    "      nn.Linear(hidden_dim, 1),\n",
    "      nn.ELU()\n",
    "    )\n",
    "\n",
    "  def forward(self, data):\n",
    "    x, edge_index = data.x, data.edge_index\n",
    "\n",
    "    # Node embedding\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.convs[i](x, edge_index)\n",
    "      x = torch.relu(x)\n",
    "      x = torch.dropout(x, p=0.1, train=self.training)\n",
    "\n",
    "    # Edge probability prediction\n",
    "    row, col = edge_index\n",
    "    edge_features = torch.cat([x[row], x[col]], dim=1)\n",
    "    edge_probs = torch.sigmoid(self.edge_predictor(edge_features).squeeze())\n",
    "    #edge_probs = torch.sigmoid(torch.sum(x[row] * x[col], dim=1))\n",
    "\n",
    "    return edge_probs\n",
    "    \n",
    "class CascadeDataset(Dataset):\n",
    "  def __init__(self, cascades):\n",
    "    self.cascades = cascades\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.cascades)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.cascades[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cascade_likelihood(edge_probs, edge_index, cascade, epsilon=1e-8):\n",
    "  \"\"\"\n",
    "  Compute the likelihood of observing a single cascade given edge probabilities.\n",
    "  \n",
    "  Args:\n",
    "  - edge_probs: Tensor of predicted edge probabilities\n",
    "  - edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "  - cascade: List of lists, where each inner list contains nodes activated at that time step\n",
    "  - epsilon: Small value to avoid log(0)\n",
    "  \n",
    "  Returns:\n",
    "  - log_likelihood: Log-likelihood of the cascade\n",
    "  \"\"\"\n",
    "  device = edge_probs.device\n",
    "  num_nodes = edge_index.max().item() + 1\n",
    "  activated = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "  log_likelihood = 0.0\n",
    "\n",
    "  src, dst = edge_index\n",
    "\n",
    "  for t in range(len(cascade)):\n",
    "    prev_activated = torch.tensor(cascade[t-1] if t-1 >= 0 else [], device=device)\n",
    "    curr_activated = torch.tensor(cascade[t], device=device)\n",
    "    next_activated = torch.tensor(cascade[t+1] if t+1 < len(cascade) else [], device=device)\n",
    "    activated[curr_activated] = True\n",
    "    \n",
    "    # Probability of activation from parents\n",
    "    for v in curr_activated:\n",
    "      parents = src[(dst == v) & activated[src]]\n",
    "      activated_parents = parents[torch.isin(parents, prev_activated)]\n",
    "      if len(activated_parents) > 0:\n",
    "        prob_v_activated = 1 - torch.prod(1 - edge_probs[torch.isin(src, activated_parents) & (dst == v)])\n",
    "        log_likelihood += torch.log(prob_v_activated + epsilon)\n",
    "\n",
    "    # Probability of non-activation of children\n",
    "    for v in curr_activated:\n",
    "      children = dst[(src == v) & ~activated[dst]]\n",
    "      non_activated_children = children[~torch.isin(children, next_activated)]\n",
    "      if len(non_activated_children) > 0:\n",
    "        prob_children_not_activated = torch.prod(1 - edge_probs[(src == v) & torch.isin(dst, non_activated_children)])\n",
    "        log_likelihood += torch.log(prob_children_not_activated + epsilon)\n",
    "\n",
    "  return log_likelihood\n",
    "\n",
    "  '''\n",
    "  device = edge_probs.device\n",
    "  num_nodes = edge_index.max().item() + 1\n",
    "  activated = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "  log_likelihood = 0.0\n",
    "\n",
    "  for t, activated_nodes in enumerate(cascade):\n",
    "    if t == 0:\n",
    "      activated[activated_nodes] = True\n",
    "      continue\n",
    "\n",
    "    # Compute activation probabilities for this step\n",
    "    src, dst = edge_index\n",
    "    mask = activated[src] & ~activated[dst]\n",
    "    relevant_probs = edge_probs[mask]\n",
    "    relevant_dst = dst[mask]\n",
    "\n",
    "    # Compute likelihood of activations and non-activations\n",
    "    new_activations = torch.tensor(activated_nodes, device=device)\n",
    "    activated_probs = relevant_probs[torch.isin(relevant_dst, new_activations)]\n",
    "    non_activated_probs = relevant_probs[~torch.isin(relevant_dst, new_activations)]\n",
    "\n",
    "    log_likelihood += torch.sum(torch.log(activated_probs + epsilon))\n",
    "    log_likelihood += torch.sum(torch.log(1 - non_activated_probs + epsilon))\n",
    "\n",
    "    # Update activated nodes\n",
    "    activated[activated_nodes] = True\n",
    "\n",
    "  return log_likelihood\n",
    "  '''\n",
    "\n",
    "def compute_loss(edge_probs, edge_index, cascades):\n",
    "  \"\"\"\n",
    "  Compute the negative log-likelihood loss for multiple cascades.\n",
    "  \n",
    "  Args:\n",
    "  - edge_probs: Tensor of predicted edge probabilities\n",
    "  - edge_index: Tensor of shape [2, num_edges] containing edge indices\n",
    "  - cascades: List of cascades, where each cascade is a list of lists of activated nodes\n",
    "  \n",
    "  Returns:\n",
    "  - loss: Negative log-likelihood loss\n",
    "  \"\"\"\n",
    "  total_log_likelihood = 0.0\n",
    "  for cascade in cascades:\n",
    "    total_log_likelihood += compute_cascade_likelihood(edge_probs, edge_index, cascade)\n",
    "  \n",
    "  # Return negative log-likelihood as the loss\n",
    "  return -total_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(G: nx.DiGraph, features):\n",
    "  # Create a PyG Data object from the networkx graph\n",
    "  edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "  #x = torch.tensor(features, dtype=torch.float)\n",
    "  data = Data(x=features, edge_index=edge_index)\n",
    "  return data\n",
    "\n",
    "n = 100\n",
    "p = 0.1\n",
    "gname = f\"er_{n}_{str(p).replace('.', '')}\"\n",
    "path = Path(f\"datasets/synthetic/{gname}\")\n",
    "\n",
    "with open(path / f\"{gname}.mtx\", \"rb\") as fh:\n",
    "  G = nx.from_scipy_sparse_array(sp.io.mmread(fh), create_using=nx.DiGraph)\n",
    "with open(path / \"feats.npy\", \"rb\") as fh:\n",
    "  features_npy = torch.tensor(np.load(fh), dtype=torch.float)\n",
    "\n",
    "cascades = []\n",
    "idxes = rng.choice(500, 100, replace=False)\n",
    "for i in idxes:\n",
    "  with open(path / f\"diffusions/timestamps/{i}.txt\", \"r\") as fh:\n",
    "    cascade = []\n",
    "    for line in fh:\n",
    "      cascade.append(list(map(int, line.strip().split())))\n",
    "    cascades.append(cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Total Loss: 13293.3374\n",
      "Epoch 11/100, Total Loss: 11944.2715\n",
      "Epoch 21/100, Total Loss: 11815.2622\n",
      "Epoch 31/100, Total Loss: 11790.8296\n",
      "Epoch 41/100, Total Loss: 11790.7979\n",
      "Epoch 51/100, Total Loss: 11784.6089\n",
      "Epoch 61/100, Total Loss: 11782.4673\n",
      "Epoch 71/100, Total Loss: 11778.4082\n",
      "Epoch 81/100, Total Loss: 11770.3862\n",
      "Epoch 91/100, Total Loss: 11757.4048\n",
      "Epoch 100/100, Total Loss: 11732.8413\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, optimizer, data, cascades, num_epochs, batch_size = 50):\n",
    "  model.train()\n",
    "  #batches = DataLoader(cascades, batch_size=10, shuffle=True)\n",
    "  #print(batches)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    loss = 0.0\n",
    "    rng.shuffle(cascades)\n",
    "    batches = [cascades[i:i+batch_size] for i in range(0, len(cascades), batch_size)]\n",
    "\n",
    "    for batch in batches:\n",
    "      optimizer.zero_grad()\n",
    "      edge_probs = model.forward(data)\n",
    "\n",
    "      batch_loss = compute_loss(edge_probs, data.edge_index, batch)\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      loss += batch_loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Total Loss: {loss:.4f}\")\n",
    "      #print(edge_probs[0])\n",
    "      #print(edge_probs[1])\n",
    "      #print('\\n')\n",
    "\n",
    "features_eye = torch.eye(G.number_of_nodes())\n",
    "data = create_dataset(G, features_eye)\n",
    "model = GNNIndependentCascade(data.num_features, 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "train_model(model, optimizer, data, cascades, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.68733220664117\n",
      "0.19553091420377325\n",
      "tensor([0.3130, 0.3316, 0.3023, 0.3137, 0.3223, 0.3287, 0.3517, 0.3292, 0.3487,\n",
      "        0.3243, 0.3074, 0.3337, 0.3208, 0.3147, 0.3186, 0.3236, 0.3267, 0.3151,\n",
      "        0.3063, 0.3131, 0.2978, 0.2986, 0.2992, 0.2973, 0.3013, 0.2907, 0.3048,\n",
      "        0.2893, 0.2945, 0.2914, 0.2923, 0.3044, 0.3038, 0.2982, 0.2981, 0.3100,\n",
      "        0.3063, 0.3125, 0.3108, 0.3281, 0.3000, 0.3360, 0.3293, 0.3127, 0.3456,\n",
      "        0.3186, 0.3429, 0.2895, 0.2868, 0.2925, 0.2868, 0.2857, 0.2868, 0.2919,\n",
      "        0.3024, 0.3148, 0.3226, 0.3259, 0.3138, 0.3263, 0.3199, 0.3425, 0.3090,\n",
      "        0.3180, 0.3378, 0.3296, 0.3035, 0.3245, 0.3183, 0.2897, 0.3025, 0.3230,\n",
      "        0.3011, 0.3040, 0.3204, 0.3114, 0.3144, 0.3110, 0.3033, 0.2991, 0.3038,\n",
      "        0.3179, 0.3385, 0.3564, 0.3123, 0.3280, 0.3373, 0.3131, 0.3244, 0.3216,\n",
      "        0.3153, 0.3072, 0.3215, 0.3228, 0.3325, 0.3475, 0.3306, 0.3317, 0.3495,\n",
      "        0.3421, 0.3561, 0.3265, 0.3030, 0.3384, 0.3041, 0.3262, 0.3357, 0.3467,\n",
      "        0.3382, 0.3402, 0.3423, 0.3274, 0.3320, 0.3210, 0.3143, 0.3366, 0.3227,\n",
      "        0.3123, 0.3021, 0.3167, 0.3160, 0.3018, 0.3003, 0.3001, 0.2942, 0.2965,\n",
      "        0.3007, 0.3056, 0.3114, 0.3249, 0.3072, 0.3122, 0.2952, 0.3114, 0.3118,\n",
      "        0.3040, 0.3042, 0.3077, 0.2984, 0.2997, 0.3206, 0.3135, 0.3189, 0.3303,\n",
      "        0.2978, 0.3065, 0.3019, 0.3168, 0.3085, 0.3085, 0.3059, 0.3039, 0.2974,\n",
      "        0.3129, 0.3269, 0.2961, 0.3142, 0.3137, 0.3169, 0.3128, 0.2969, 0.3137,\n",
      "        0.3150, 0.3114, 0.3293, 0.2948, 0.3216, 0.2948, 0.3119, 0.3205, 0.3107,\n",
      "        0.3115, 0.3179, 0.3180, 0.3127, 0.3197, 0.3050, 0.3223, 0.3062, 0.3174,\n",
      "        0.3072, 0.2962, 0.3151, 0.2996, 0.3155, 0.3331, 0.3196, 0.3490, 0.3182,\n",
      "        0.3290, 0.3352, 0.3542, 0.3374, 0.3417, 0.3352, 0.3254, 0.3180, 0.3085,\n",
      "        0.3237, 0.3142, 0.3222, 0.3125, 0.3336, 0.3422, 0.3535, 0.3519, 0.3261,\n",
      "        0.3213, 0.3415, 0.3060, 0.3104, 0.3160, 0.3616, 0.3217, 0.3009, 0.3398,\n",
      "        0.3158, 0.3222, 0.3123, 0.3041, 0.2931, 0.3007, 0.3026, 0.2969, 0.3011,\n",
      "        0.2948, 0.2938, 0.2977, 0.3246, 0.3235, 0.3173, 0.3386, 0.3459, 0.3533,\n",
      "        0.3549, 0.3323, 0.3336, 0.3325, 0.3622, 0.3000, 0.3128, 0.3195, 0.3021,\n",
      "        0.3065, 0.3107, 0.3021, 0.3011, 0.3264, 0.3281, 0.3244, 0.3219, 0.3205,\n",
      "        0.3141, 0.3106, 0.3160, 0.2996, 0.3156, 0.3251, 0.3419, 0.3091, 0.3466,\n",
      "        0.3324, 0.3285, 0.3098, 0.3088, 0.3107, 0.3215, 0.3262, 0.3219, 0.3068,\n",
      "        0.3083, 0.3245, 0.3145, 0.3075, 0.3055, 0.3124, 0.3046, 0.3233, 0.3215,\n",
      "        0.3104, 0.3009, 0.3163, 0.3111, 0.2991, 0.3047, 0.2991, 0.3221, 0.2943,\n",
      "        0.3138, 0.3182, 0.3059, 0.3044, 0.3090, 0.3116, 0.3091, 0.3100, 0.3018,\n",
      "        0.3187, 0.2951, 0.2890, 0.3038, 0.3095, 0.3041, 0.3017, 0.2946, 0.3024,\n",
      "        0.2980, 0.3084, 0.3062, 0.3117, 0.3109, 0.2889, 0.3023, 0.3037, 0.2964,\n",
      "        0.3019, 0.3071, 0.2886, 0.3168, 0.3045, 0.3108, 0.3090, 0.3074, 0.2972,\n",
      "        0.3121, 0.3014, 0.3082, 0.3263, 0.3297, 0.3098, 0.3284, 0.3306, 0.3148,\n",
      "        0.3216, 0.3323, 0.3096, 0.3113, 0.3063, 0.3450, 0.3380, 0.3269, 0.3328,\n",
      "        0.3391, 0.3460, 0.3200, 0.3309, 0.3348, 0.3334, 0.3090, 0.3176, 0.3133,\n",
      "        0.3265, 0.3342, 0.3437, 0.3152, 0.3426, 0.3277, 0.3207, 0.3300, 0.3218,\n",
      "        0.3326, 0.3295, 0.3076, 0.3435, 0.3580, 0.3485, 0.3775, 0.3424, 0.3314,\n",
      "        0.3468, 0.3295, 0.3321, 0.3122, 0.3297, 0.3433, 0.3408, 0.3579, 0.3385,\n",
      "        0.3431, 0.3146, 0.3137, 0.3293, 0.3169, 0.3377, 0.3007, 0.3197, 0.3121,\n",
      "        0.3302, 0.3345, 0.3362, 0.3302, 0.3343, 0.3112, 0.3382, 0.2984, 0.3115,\n",
      "        0.3279, 0.3317, 0.3123, 0.3240, 0.3016, 0.3234, 0.2995, 0.3224, 0.3176,\n",
      "        0.3134, 0.3251, 0.3516, 0.3161, 0.3002, 0.3128, 0.3576, 0.3064, 0.3113,\n",
      "        0.3407, 0.3110, 0.3314, 0.3132, 0.2973, 0.3139, 0.3260, 0.3291, 0.3100,\n",
      "        0.3271, 0.3145, 0.3223, 0.3224, 0.3296, 0.3210, 0.3170, 0.3034, 0.3096,\n",
      "        0.3066, 0.2941, 0.3055, 0.2986, 0.2981, 0.3087, 0.3060, 0.3385, 0.3165,\n",
      "        0.3115, 0.3167, 0.2982, 0.3045, 0.3273, 0.3221, 0.3022, 0.3158, 0.3024,\n",
      "        0.3112, 0.3073, 0.3302, 0.3116, 0.3100, 0.3281, 0.3025, 0.3230, 0.3258,\n",
      "        0.3192, 0.2979, 0.3161, 0.2989, 0.3119, 0.3002, 0.3278, 0.3191, 0.3170,\n",
      "        0.3365, 0.3051, 0.3248, 0.2960, 0.3128, 0.3206, 0.3067, 0.2989, 0.3027,\n",
      "        0.2965, 0.3020, 0.2937, 0.3034, 0.2972, 0.3016, 0.3292, 0.3095, 0.3098,\n",
      "        0.3089, 0.3255, 0.3306, 0.2978, 0.3178, 0.3499, 0.3158, 0.3369, 0.3350,\n",
      "        0.3355, 0.3198, 0.3210, 0.3302, 0.3304, 0.3192, 0.3127, 0.3062, 0.3253,\n",
      "        0.3179, 0.3138, 0.3331, 0.3097, 0.3290, 0.3340, 0.2987, 0.3041, 0.3174,\n",
      "        0.3091, 0.3317, 0.3336, 0.3165, 0.3227, 0.3205, 0.3022, 0.3313, 0.3127,\n",
      "        0.2993, 0.3308, 0.3076, 0.3149, 0.3079, 0.3082, 0.3243, 0.3087, 0.3130,\n",
      "        0.3074, 0.3237, 0.3052, 0.3143, 0.3086, 0.3212, 0.3053, 0.3050, 0.3118,\n",
      "        0.2914, 0.3300, 0.3234, 0.3171, 0.3151, 0.3018, 0.3059, 0.3016, 0.3003,\n",
      "        0.3309, 0.3035, 0.3129, 0.3080, 0.3278, 0.3294, 0.3311, 0.3187, 0.2998,\n",
      "        0.3247, 0.3283, 0.3191, 0.3038, 0.3070, 0.3255, 0.2998, 0.2959, 0.3204,\n",
      "        0.3137, 0.3307, 0.3514, 0.3390, 0.3219, 0.3207, 0.3337, 0.3101, 0.3050,\n",
      "        0.3071, 0.3160, 0.3063, 0.2967, 0.3071, 0.2892, 0.2927, 0.3002, 0.3161,\n",
      "        0.3254, 0.3401, 0.3148, 0.3031, 0.3236, 0.3248, 0.3249, 0.3151, 0.3170,\n",
      "        0.3007, 0.3135, 0.3143, 0.3024, 0.3032, 0.3002, 0.2918, 0.3032, 0.2976,\n",
      "        0.3048, 0.3020, 0.2955, 0.3119, 0.3002, 0.3071, 0.3159, 0.3036, 0.2937,\n",
      "        0.2959, 0.2964, 0.3020, 0.3159, 0.3073, 0.3145, 0.3011, 0.3178, 0.3184,\n",
      "        0.3065, 0.3147, 0.3039, 0.3003, 0.3101, 0.3166, 0.3007, 0.2959, 0.3024,\n",
      "        0.3142, 0.2972, 0.3332, 0.3129, 0.3092, 0.3193, 0.3124, 0.3308, 0.3177,\n",
      "        0.3199, 0.3188, 0.3096, 0.3189, 0.3056, 0.3213, 0.3260, 0.3332, 0.2963,\n",
      "        0.3050, 0.3223, 0.3223, 0.3055, 0.2983, 0.3044, 0.3229, 0.3228, 0.3208,\n",
      "        0.3081, 0.3291, 0.3092, 0.3313, 0.3166, 0.3142, 0.3088, 0.3020, 0.3111,\n",
      "        0.3074, 0.3446, 0.3320, 0.3360, 0.3318, 0.3178, 0.3149, 0.3168, 0.3241,\n",
      "        0.3226, 0.2998, 0.3133, 0.3286, 0.3267, 0.3258, 0.2965, 0.3037, 0.3157,\n",
      "        0.3084, 0.3168, 0.3006, 0.3237, 0.2912, 0.2862, 0.2954, 0.2902, 0.2902,\n",
      "        0.2928, 0.3157, 0.3218, 0.3134, 0.3065, 0.3179, 0.3180, 0.3093, 0.3274,\n",
      "        0.3083, 0.3060, 0.3256, 0.3064, 0.3158, 0.3134, 0.3068, 0.3288, 0.3165,\n",
      "        0.2979, 0.3249, 0.3162, 0.3089, 0.3257, 0.3207, 0.2998, 0.2937, 0.3162,\n",
      "        0.3169, 0.3080, 0.3108, 0.3159, 0.3345, 0.3285, 0.3138, 0.3276, 0.3055,\n",
      "        0.3239, 0.3134, 0.3073, 0.3119, 0.3227, 0.3182, 0.3127, 0.3153, 0.3042,\n",
      "        0.3206, 0.3303, 0.3001, 0.3076, 0.3119, 0.3029, 0.3067, 0.3060, 0.3053,\n",
      "        0.3060, 0.3129, 0.3373, 0.3104, 0.3229, 0.2979, 0.3072, 0.3112, 0.3174,\n",
      "        0.3169, 0.3077, 0.3074, 0.3082, 0.3036, 0.3532, 0.3516, 0.3176, 0.3122,\n",
      "        0.3392, 0.3362, 0.3076, 0.3225, 0.3069, 0.3199, 0.3243, 0.3268, 0.3141,\n",
      "        0.3232, 0.3136, 0.3146, 0.3129, 0.2942, 0.3008, 0.2991, 0.2909, 0.3160,\n",
      "        0.3076, 0.3356, 0.3094, 0.3287, 0.3153, 0.3115, 0.3063, 0.3309, 0.3138,\n",
      "        0.3482, 0.3158, 0.3086, 0.3229, 0.3355, 0.2999, 0.2984, 0.2997, 0.3018,\n",
      "        0.3030, 0.2923, 0.2989, 0.2994, 0.3042, 0.2936, 0.3103, 0.3079, 0.2910,\n",
      "        0.3068, 0.3083, 0.3076, 0.3035, 0.2942, 0.3058, 0.3005, 0.2895, 0.3046,\n",
      "        0.2944, 0.3045, 0.3164, 0.3019, 0.3171, 0.3033, 0.3173, 0.3076, 0.3061,\n",
      "        0.3073, 0.3160, 0.3231, 0.3271, 0.3192, 0.3136, 0.3227, 0.3351, 0.3040,\n",
      "        0.3087, 0.3220, 0.3150, 0.3222, 0.3190, 0.3033, 0.3095, 0.3015, 0.3087,\n",
      "        0.3171, 0.2979, 0.3073, 0.3416, 0.3100, 0.3244, 0.3322, 0.3185, 0.3185,\n",
      "        0.3307, 0.3357, 0.3348, 0.3605, 0.3305, 0.3390, 0.3492, 0.3114, 0.3006,\n",
      "        0.3057, 0.3160, 0.3135, 0.3101, 0.3069, 0.3320, 0.3109, 0.3287, 0.3048,\n",
      "        0.3194, 0.3242, 0.3194, 0.3208, 0.3100, 0.3052, 0.2981, 0.3039, 0.3199,\n",
      "        0.3222, 0.3392, 0.2988, 0.3304, 0.3000, 0.3017, 0.3128, 0.3082, 0.3159,\n",
      "        0.3177, 0.3270, 0.3414, 0.3249, 0.3115, 0.3012, 0.3079, 0.3248, 0.3019,\n",
      "        0.3453, 0.3101, 0.3385, 0.3582, 0.3213, 0.3438, 0.2954, 0.3206, 0.3005,\n",
      "        0.3102, 0.3238, 0.3110, 0.3062, 0.2950, 0.3169, 0.3215, 0.3299, 0.3335,\n",
      "        0.2955, 0.3105, 0.3268, 0.3231, 0.3219, 0.3243, 0.3217, 0.3120, 0.3330,\n",
      "        0.3310, 0.3282, 0.3323, 0.3188, 0.3073, 0.3114, 0.3177, 0.3119, 0.3209,\n",
      "        0.3005, 0.3086, 0.3298, 0.3103, 0.3263, 0.3518, 0.3131, 0.3205, 0.3002,\n",
      "        0.3104, 0.3364, 0.3096, 0.3113, 0.3024, 0.2994, 0.3030, 0.3067, 0.3059,\n",
      "        0.3060, 0.3035, 0.3000, 0.2932, 0.3017, 0.2983, 0.3421, 0.3153, 0.3082,\n",
      "        0.3017, 0.3243], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "l1_error = 0\n",
    "l2_error = 0\n",
    "edge_probs = model(data)\n",
    "\n",
    "for i, e in enumerate(G.edges()):\n",
    "  u, v = e\n",
    "  p = G[u][v]['weight']\n",
    "  l1_error += abs(p - edge_probs[i].item())\n",
    "  l2_error += (p - edge_probs[i].item())**2\n",
    "\n",
    "print(l1_error)\n",
    "print(l1_error / G.number_of_edges())\n",
    "print(edge_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
